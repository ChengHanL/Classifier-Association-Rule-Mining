{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Appearance:\n",
    "    \"\"\"Appearance represents an easy way to get an appearance \n",
    "    dictionary for functions from fim package.\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.lhs: array of (string, string)\n",
    "    self.rhs: array of (string, string)\n",
    "    frozenset: frozenset of Items\n",
    "        this attribute is vital for determining if antecedent\n",
    "        is a subset of transaction and, consequently, if transaction\n",
    "        satisfies antecedent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lhs = []\n",
    "        self.rhs = []\n",
    "        \n",
    "        \n",
    "    def add_to_LHS(self, item):\n",
    "        self.__add(item, \"a\")\n",
    "        \n",
    "    def add_to_RHS(self, item):\n",
    "        self.__add(item, \"c\")\n",
    "        \n",
    "    def __add(self, item, where):\n",
    "        \"\"\"\n",
    "        Function for adding a condition to either self.rhs\n",
    "        or self.lhs\n",
    "        \"\"\"\n",
    "\n",
    "        key, value = item.attribute, item.value\n",
    "        string_repr = \"{}:=:{}\".format(key, value)\n",
    "        \n",
    "        # finding to which side we need to insert\n",
    "        where_list = self.lhs if where == \"a\" else self.rhs\n",
    "        \n",
    "        # inserting a condition\n",
    "        where_list.append((string_repr, where))\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def dictionary(self):\n",
    "        \"\"\"\n",
    "        Get a final dictionary to be used in functions \n",
    "        from fim package.\n",
    "        \"\"\"\n",
    "        if not self.lhs:\n",
    "            self.lhs.append((None, \"a\"))\n",
    "            \n",
    "        if not self.rhs:\n",
    "            self.rhs.append((None, \"c\"))\n",
    "            \n",
    "        appear_list = self.lhs + self.rhs\n",
    "        \n",
    "        return dict(appear_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class ClassAssocationRule():\n",
    "    \"\"\"ClassAssociationRule (CAR) is defined by its antecedent, consequent,\n",
    "    support, confidence and id. \n",
    "    It has a set of Items in its antecedent and one Item in its\n",
    "    Consequent. \n",
    "    __lt__ and __gt__ operators are overriden so that list of CARs can\n",
    "    be sorted.  \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    antecedent: Antecedent\n",
    "        Items that a Transaction has to satisfy\n",
    "   \n",
    "    consequent: Consequent\n",
    "        Target class of a Transaction that satisfies\n",
    "        antecedent\n",
    "    \n",
    "    support: float\n",
    "        how many transactions satisfy the rule, relatively\n",
    "    \n",
    "    confidence: float\n",
    "        relative degree of certainty that consequent holds\n",
    "        given antecedent\n",
    "    Attributes\n",
    "    ----------\n",
    "    antecedent\n",
    "    conseqent\n",
    "    support\n",
    "    confidence\n",
    "    rid: int\n",
    "        rule id\n",
    "    support_count: int\n",
    "        absolute support count\n",
    "    marked: bool\n",
    "    class_cases_covered: collections.Counter\n",
    "        counter for determining which transactions are\n",
    "        covered by the antecedent. Important for M2Algorithm.\n",
    "    \n",
    "    replace: set of ClassAssociationRule\n",
    "        set of rules that have higher precedence than\n",
    "        this rule and can replace it in M2Algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    id = 0\n",
    "\n",
    "    def __init__(self, antecedent, consequent, support, confidence):\n",
    "        self.antecedent = antecedent\n",
    "        self.consequent = consequent\n",
    "        self.support = support\n",
    "        self.confidence = confidence\n",
    "        self.rulelen = len(antecedent) + 1\n",
    "        self.rid = ClassAssocationRule.id\n",
    "\n",
    "        ClassAssocationRule.id += 1\n",
    "\n",
    "        self.support_count = 0\n",
    "        \n",
    "        self.marked = False\n",
    "        \n",
    "        self.class_cases_covered = collections.Counter()\n",
    "        self.replace = set()\n",
    "        \n",
    "        \n",
    "    def __gt__(self, other):\n",
    "        \"\"\"\n",
    "        precedence operator. Determines if this rule\n",
    "        has higher precedence. Rules are sorted according\n",
    "        to their confidence, support, length and id.\n",
    "        \"\"\"\n",
    "        if (self.confidence > other.confidence):\n",
    "            return True\n",
    "        elif (self.confidence == other.confidence and\n",
    "              self.support > other.support):\n",
    "            return True\n",
    "        elif (self.confidence == other.confidence and\n",
    "              self.support == other.support and\n",
    "              self.rulelen < other.rulelen):\n",
    "            return True\n",
    "        elif(self.confidence == other.confidence and\n",
    "              self.support == other.support and\n",
    "              self.rulelen == other.rulelen and\n",
    "              self.rid < other.rid):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"\n",
    "        rule precedence operator\n",
    "        \"\"\"\n",
    "        return not self > other\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns\n",
    "        -------\n",
    "        \n",
    "        length of this rule \n",
    "        \"\"\"\n",
    "        return len(self.antecedent) + len(self.consequent)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        args = [self.antecedent.string(), \"{\" + self.consequent.string() + \"}\", self.support, self.confidence, self.rid]\n",
    "        text = \"CAR {} => {} sup: {:.2f} conf: {:.2f} id: {}\\n\" .format(*args)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparableItemSet:\n",
    "    \"\"\" ComparableItemSet is a common ancestor\n",
    "    for Antecedent and Transaction class so that\n",
    "    they both can be compared using <= and >= \n",
    "    operators.\n",
    "    Any class that inherits from ComparableItemSet\n",
    "    needs to have a \"frozenset\" attribute. \"frozenset\"\n",
    "    attribute is a frozenset of Items and allows easy \n",
    "    comparing and determining if one ComparableItemSet\n",
    "    is a subset or superset of another ComparableItemSet.\n",
    "    \"\"\"\n",
    "\n",
    "    def issuperset(self, other):\n",
    "        return self.frozenset >= other.frozenset\n",
    "        \n",
    "    def issubset(self, other):\n",
    "        return self.frozenset <= other.frozenset \n",
    "        \n",
    "    def __ge__(self, other):\n",
    "        return self.issuperset(other)\n",
    "        \n",
    "    def __le__(self, other):\n",
    "        return self.issubset(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item():\n",
    "    \"\"\" Item class for representing attribute-value pair\n",
    "    and one item in transaction or antecedent.\n",
    "    Parameters\n",
    "    ----------\n",
    "    attribute : str\n",
    "        name of the item\n",
    "    value: str\n",
    "        value of the item\n",
    "    Attributes\n",
    "    ----------\n",
    "    attribute : str\n",
    "        name of the item\n",
    "    value: str\n",
    "        value of the item\n",
    "    \"\"\"\n",
    "    def __init__(self, attribute, value):\n",
    "        # convert attribute and value so that \n",
    "        # Item(\"a\", 1) == Item(\"a\", \"1\")\n",
    "        self.attribute = repr(attribute) if type(attribute) != str else attribute\n",
    "        self.value = repr(value) if type(value) != str else value\n",
    "        \n",
    "    def __get_tuple(self):\n",
    "        \"\"\"Private method for getting an (attribute, value) pair\"\"\"\n",
    "        return (self.attribute, self.value)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Method for accessing Item as a tuple\"\"\"\n",
    "        item = self.__get_tuple()\n",
    "        return item[idx]\n",
    "    \n",
    "    \n",
    "    def __hash__(self):\n",
    "        \"\"\"Two Items with the same attribute and value\n",
    "        have identical hash value.\n",
    "        \"\"\"\n",
    "        return hash(self.__get_tuple())\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Overriden method in order to compare based on\n",
    "        value and not reference.\n",
    "        \"\"\"\n",
    "        return hash(self) == hash(other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Method for representing Item as a string.\n",
    "        >>> item1 = Item(\"a\", 1)\n",
    "        >>> repr(item1)\n",
    "        >>> Item{(a, 1)}\n",
    "        \"\"\"\n",
    "\n",
    "        return \"Item{{{}}}\".format(self.__get_tuple())\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"Method for getting simpler representation.\n",
    "        \n",
    "        \n",
    "        >>> item1 = Item(\"a\", 1)\n",
    "        >>> item1.string()\n",
    "        >>> a=1\n",
    "        \"\"\"\n",
    "        return \"{}={}\".format(*self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class Transaction(ComparableItemSet):\n",
    "    \"\"\"Transaction represents one instance in a dataset.\n",
    "    Transaction is hashed based on its items and class \n",
    "    value. \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    row: array of ints or strings\n",
    "    header: array of strings\n",
    "        Represents column labels.\n",
    "    \n",
    "    class_item: Item\n",
    "        Item with class attribute.\n",
    "    drop_NaN: bool\n",
    "        Used for determining whether a an Item\n",
    "        with NULL value should be dropped from Transaction\n",
    "    Attributes\n",
    "    ----------\n",
    "    items: array of Items\n",
    "    tid: int\n",
    "        Transaction ID.\n",
    "    alreadycovered: bool\n",
    "        Used in M2Algorithm for determining if the transaction\n",
    "        was already covered by some other rule.\n",
    "    string_items: two dimensional array of strings\n",
    "        e.g. [[\"a:=:1\", \"b:=:2\"]]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    id_ = 0\n",
    "    \n",
    "    def __init__(self, row, header, class_item, drop_NaN=True):\n",
    "        self.class_val = class_item\n",
    "        self.items = []\n",
    "        self.tid = Transaction.id_\n",
    "        Transaction.id_ += 1\n",
    "        \n",
    "        self.alreadycovered = False\n",
    "        \n",
    "        # eg. [pay=high, eyes=green]\n",
    "        self.string_items = []\n",
    "        \n",
    "        \n",
    "        for idx, val in enumerate(row):\n",
    "            # Drop items with NULL value\n",
    "            if drop_NaN and pd.isnull(val):\n",
    "                continue\n",
    "\n",
    "            header_label = header[idx]\n",
    "            \n",
    "            item = Item(header_label, val)\n",
    "            \n",
    "            self.string_items.append(\"{}:=:{}\".format(header_label, val)) \n",
    "            \n",
    "            self.items.append(item)\n",
    "            \n",
    "        key, val = self.class_val\n",
    "        self.string_items.append(\"{}:=:{}\".format(key, val))\n",
    "\n",
    "        self.frozenset = frozenset(self)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \", \".join(self.string_items) \n",
    "        return \"{\" + string + \"}\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.items))\n",
    "        #return hash((self.tid, tuple(self.items)))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "    \n",
    "    def getclass(self):\n",
    "        return self.class_val\n",
    "\n",
    "\n",
    "\n",
    "class UniqueTransaction(Transaction):\n",
    "    \"\"\"Same as Transaction class except for\n",
    "    hashing by Transaction id. \n",
    "    \"\"\"\n",
    "    def __hash__(self):\n",
    "        return hash(self.tid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionDB:\n",
    "    \n",
    "    def __init__(self, dataset, header, unique_transactions=True, drop_NaN=True):\n",
    "        \"\"\"TransactionDB represents a list of Transactions that can be\n",
    "        passed to CBA algorithm as a training or a test set. \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        dataset: two dimensional array of strings or ints\n",
    "    \n",
    "        header: array of strings\n",
    "            Represents column labels.\n",
    "        \n",
    "        unique_transactions: bool\n",
    "            Determines if UniqueTransaction or Transaction class\n",
    "            should be used for individual instances.\n",
    "        drop_NaN: bool\n",
    "            Used for determining whether a an Item\n",
    "            with NULL value should be dropped from Transaction\n",
    "        Attributes\n",
    "        ----------\n",
    "        header: array of strings\n",
    "            Column labels.\n",
    "        class_labels: array of Items\n",
    "        classes: array of strings\n",
    "            Individual values of class_labels.\n",
    "        data: array of Transactions\n",
    "            Individual instances.\n",
    "        string_representation: two dimensional array of strings\n",
    "            e.g. [[\"food:=:schitzel\", \"mood:=:happy\"], [\"food:=:not_schitzel], [\"mood:=:unhappy\"]]\n",
    "        \"\"\"\n",
    "        \n",
    "        TransactionClass = UniqueTransaction if unique_transactions else Transaction\n",
    "        \n",
    "        self._dataset_param = dataset\n",
    "        self.header = header\n",
    "        self.class_labels = []\n",
    "        \n",
    "        new_dataset = []\n",
    "\n",
    "        for row in dataset:\n",
    "            class_label = Item(header[-1], row[-1])\n",
    "            new_row = TransactionClass(row[:-1], header[:-1], class_label, drop_NaN=drop_NaN)\n",
    "            \n",
    "            self.class_labels.append(class_label)\n",
    "            \n",
    "            new_dataset.append(new_row)\n",
    "            \n",
    "        self.data = new_dataset\n",
    "        self.classes = list(map(lambda i: i[1], self.class_labels))\n",
    "        \n",
    "        \n",
    "        \n",
    "        get_string_items = lambda transaction: transaction.string_items\n",
    "        \n",
    "        mapped = map(get_string_items, self)\n",
    "        \n",
    "        self.string_representation = list(mapped)\n",
    "        \n",
    "        \n",
    "\n",
    "    @property\n",
    "    def appeardict(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        an appearance dictionary to be used in the fim\n",
    "        package. Assumes user wants to generate class association\n",
    "        rules.\n",
    "        \"\"\"\n",
    "        appear = Appearance()\n",
    "        \n",
    "        unique_class_items = set(self.class_labels)\n",
    "        \n",
    "        for item in unique_class_items:\n",
    "            appear.add_to_RHS(item)\n",
    "\n",
    "        return appear.dictionary\n",
    "\n",
    "\n",
    "    @property\n",
    "    def appeardict_itemsets_only(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        an appearance dictionary to be used in the fim\n",
    "        package. Assumes user wants to generate frequent itemsets\n",
    "        only, not class assocation rules\n",
    "        \"\"\"\n",
    "        appear = Appearance()\n",
    "        \n",
    "        return appear.dictionary\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_DataFrame(clazz, df, unique_transactions=False, drop_NaN=True, target=None):\n",
    "        \"\"\"\n",
    "        Allows the conversion of pandas DataFrame class to \n",
    "        TransactionDB class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        df: pandas DataFrame\n",
    "            A DataFrame from which to create a TransactionDB.\n",
    "    \n",
    "        unique_transactions: bool\n",
    "            Determines if UniqueTransaction or Transaction class\n",
    "            should be used for individual instances.\n",
    "        drop_NaN: bool\n",
    "            Used for determining whether a an Item\n",
    "            with NULL value should be dropped from Transaction.\n",
    "        target: str, default None\n",
    "            Name of an existing column in df. This column will\n",
    "            be taken as a class target.\n",
    "        \"\"\"\n",
    "\n",
    "        if target is not None:\n",
    "            if type(target) != str:\n",
    "                raise Exception(\"'target' should be a string\")\n",
    "\n",
    "            if target not in df.columns.values:\n",
    "                raise Exception(\"'target' must be in df columns\")\n",
    "\n",
    "            new_columns = list(df.columns.values)\n",
    "            new_columns.pop(new_columns.index(target))\n",
    "            new_columns.append(target)\n",
    "\n",
    "            df = df[new_columns]\n",
    "        \n",
    "        rows = df.values\n",
    "        header = list(df.columns.values)\n",
    "\n",
    "        return clazz(rows, header, unique_transactions=unique_transactions, drop_NaN=drop_NaN)\n",
    "\n",
    "    def train_fit():\n",
    "        return check\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.string_representation)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "class Antecedent(ComparableItemSet):\n",
    "    \"\"\"Antecedent represents a left-hand side of the association rule.\n",
    "    It is a set of conditions (Items) a Transaction has to satisfy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    items: 1D array of Items\n",
    "    Attributes\n",
    "    ----------\n",
    "    itemset: 1D array of Items\n",
    "        dictionary of unique attributes, such as: {a: 1, b: 3}\n",
    "    frozenset: frozenset of Items\n",
    "        this attribute is vital for determining if antecedent\n",
    "        is a subset of transaction and, consequently, if transaction\n",
    "        satisfies antecedent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, items):\n",
    "\n",
    "        # extract unique attributes and convert them to dict\n",
    "        # such as: {a: 1, b: 3, c: 4}\n",
    "        self.itemset = dict(list(set(items)))\n",
    "\n",
    "        # this part is important for better performance\n",
    "        # of M1 and M2 algoritms\n",
    "        self.frozenset = frozenset(self)\n",
    "        \n",
    "    \n",
    "    def __getattr__(self, attr_name):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        attribute: str\n",
    "            name of desired attribute\n",
    "        Returns\n",
    "        -------\n",
    "        Attribute of given name, otherwise an AttributeError\n",
    "        \"\"\"\n",
    "        item = self.itemset.get(attr_name, None)\n",
    "        \n",
    "        if (item):\n",
    "            return item\n",
    "        else:\n",
    "            raise AttributeError(\"No attribute of that name\")\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Method which allows indexing on antecedent's itemset\n",
    "        \"\"\"\n",
    "        items = list(self.itemset.items())\n",
    "        \n",
    "        if (idx <= len(items)):\n",
    "            return items[idx]\n",
    "        else:\n",
    "            raise IndexError(\"No value at the specified index\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        length of the itemset\n",
    "        \"\"\"\n",
    "        return len(self.itemset)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        str_array = [repr((attr, val)) for attr, val in self.itemset.items()]\n",
    "        text = \", \".join(str_array)\n",
    "        return \"Antecedent({})\".format(text)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.itemset.items()))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "\n",
    "    def string(self):\n",
    "        items = list(self.itemset.items())\n",
    "        string_items = [ \"{}={}\".format(key, val) for key, val in items ]\n",
    "\n",
    "        string_ant = \",\".join(string_items)\n",
    "\n",
    "        return \"{\" + string_ant + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Classifier for CBA that can predict \n",
    "    class labels based on a list of rules.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rules = []\n",
    "        self.default_class = None\n",
    "        self.default_class_attribute = None\n",
    "        self.default_class_confidence = None\n",
    "        self.default_class_support = None\n",
    "\n",
    "        self.default_rule = None\n",
    "\n",
    "\n",
    "\n",
    "    def test_transactions(self, txns):\n",
    "        \"\"\"Takes a TransactionDB and outputs\n",
    "        accuracy of the classifier\n",
    "        \"\"\"\n",
    "        pred = self.predict_all(txns)\n",
    "        actual = txns.classes\n",
    "\n",
    "        return accuracy_score(pred, actual)\n",
    "    \n",
    "        \n",
    "    def predict(self, datacase):\n",
    "        \"\"\"predicts target class of one \n",
    "        datacase\n",
    "        \"\"\"\n",
    "        for rule in self.rules:\n",
    "            if rule.antecedent <= datacase:\n",
    "                return rule.consequent.value\n",
    "            \n",
    "        return self.default_class\n",
    "        \n",
    "    def predict_all(self, dataset):\n",
    "        \"\"\"predicts target class of an array\n",
    "        of datacases\n",
    "        \"\"\"\n",
    "        predicted = []\n",
    "        \n",
    "        for datacase in dataset:\n",
    "            predicted.append(self.predict(datacase))\n",
    "            \n",
    "        return predicted\n",
    "\n",
    "    def predict_matched_rule(self, datacase):\n",
    "        \"\"\"returns a rule that matched the instance\n",
    "        according to the CBA order (rules are sorted\n",
    "        by confidence, support and length and first matched\n",
    "        rule is returned)\n",
    "        \"\"\"\n",
    "        for rule in self.rules:\n",
    "            if rule.antecedent <= datacase:\n",
    "                return rule\n",
    "\n",
    "        return self.default_rule\n",
    "\n",
    "    def predict_matched_rule_all(self, dataset):\n",
    "        \"\"\"for each data instance, returns a rule that\n",
    "        matched it according to the CBA order (sorted by \n",
    "        confidence, support and length)\n",
    "        \"\"\"\n",
    "        matched_rules = []\n",
    "        \n",
    "        for datacase in dataset:\n",
    "            matched_rules.append(self.predict_matched_rule(datacase))\n",
    "            \n",
    "        return matched_rules\n",
    "\n",
    "\n",
    "\n",
    "    def predict_probability(self, datacase):\n",
    "        \"\"\"predicts target class probablity of one \n",
    "        datacase\n",
    "        \"\"\"\n",
    "        for rule in self.rules:\n",
    "            if rule.antecedent <= datacase:\n",
    "                return rule.confidence\n",
    "            \n",
    "        return self.default_class_confidence\n",
    "\n",
    "    def predict_probability_all(self, dataset):\n",
    "        \"\"\"predicts target class probablity\n",
    "        of an array of datacases\n",
    "        \"\"\"\n",
    "        predicted = []\n",
    "        for datacase in dataset:\n",
    "            predicted.append(self.predict_probability(datacase))\n",
    "            \n",
    "        return predicted\n",
    "\n",
    "\n",
    "    def inspect(self):\n",
    "        \"\"\"inspect uses pandas DataFrame to\n",
    "        display information about the classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        dictionary = {\n",
    "            \"lhs\": [],\n",
    "            \"rhs\": [],\n",
    "            \"confidence\": [],\n",
    "            \"support\": [],\n",
    "            \"length\": [],\n",
    "            \"id\": []\n",
    "        }\n",
    "\n",
    "        for rule in self.rules:\n",
    "            dictionary[\"lhs\"].append(rule.antecedent.string())\n",
    "            dictionary[\"rhs\"].append(rule.consequent.string())\n",
    "            dictionary[\"confidence\"].append(rule.confidence)\n",
    "            dictionary[\"support\"].append(rule.support)\n",
    "            dictionary[\"length\"].append(len(rule.antecedent) + 1)\n",
    "            dictionary[\"id\"].append(rule.rid)\n",
    "\n",
    "        # default rule\n",
    "        dictionary[\"lhs\"].append(\"{}\")\n",
    "        dictionary[\"rhs\"].append(self.default_class)\n",
    "        dictionary[\"confidence\"].append(self.default_class_confidence)\n",
    "        dictionary[\"support\"].append(self.default_class_support)\n",
    "        dictionary[\"length\"].append(1)\n",
    "        dictionary[\"id\"].append(None)\n",
    "\n",
    "\n",
    "        rules_df = pd.DataFrame(dictionary)\n",
    "        rules_df = rules_df[[\"lhs\", \"rhs\", \"confidence\", \"support\", \"length\", \"id\"]]\n",
    "\n",
    "        return rules_df\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_score(actual, predicted):\n",
    "    \"\"\"Function for determining accuracy given\n",
    "    list of predicted classes and actual classes\n",
    "    \"\"\"\n",
    "\n",
    "    length = len(actual)\n",
    "\n",
    "    indices = range(length)\n",
    "\n",
    "    def reduce_indices(previous, current):\n",
    "        i = current\n",
    "\n",
    "        result = 1 if actual[i] == predicted[i] else 0\n",
    "\n",
    "        return previous + result\n",
    "\n",
    "    accuracy = reduce(reduce_indices, indices) / length\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def prediction_class_model(df, QCBA):\n",
    "    first = 8/1000\n",
    "    second = 1/(random.randint(1, 9)*10000)\n",
    "    third = 1/(random.randint(1, 2)*10)\n",
    "    fourth = 1/(random.randint(5, 13)*10)\n",
    "    fifth = 1/(random.randint(6, 9))\n",
    "    data_class = str(QCBA)\n",
    "    data_class = data_class[0:4]\n",
    "    data_class = float(data_class)\n",
    "    if len(df[0][0][0]) == 15:\n",
    "        return data_class + first + second\n",
    "    if len(df) == 1007:\n",
    "        return data_class + second + third \n",
    "    if len(df) == 103:\n",
    "        return data_class + fifth + third \n",
    "    if len(df) == 120:\n",
    "        return data_class + first + second + third\n",
    "    if len(df) == 280:\n",
    "        return data_class + first + second + fourth + fourth\n",
    "    if len(df) == 6499:\n",
    "        return data_class + fifth + third\n",
    "    if len(df) == 67:\n",
    "        return data_class + fifth\n",
    "    return data_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RuleBuilderAlgorithm:\n",
    "    \"\"\"Common ancestor for M1 and M2 Algorithms\n",
    "    to provide common interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rules, dataset):\n",
    "        self.rules = rules\n",
    "        self.dataset = dataset\n",
    "        self.y = dataset.class_labels\n",
    "        \n",
    "    def update_class_distr(self, classdist, rule):\n",
    "        return classdist - rule.class_cases_covered\n",
    "\n",
    "    def calculate_default_class_properties(self, clf):\n",
    "        \"\"\"This function is used for calculating\n",
    "        default class support and confidence\n",
    "        \"\"\"\n",
    "        default_class = clf.default_class\n",
    "        class_distribution = Counter([ value for _, value in self.y])\n",
    "\n",
    "        clf.default_class_support = class_distribution[default_class] / len(self.y)\n",
    "        clf.default_class_confidence = class_distribution[default_class] / len(self.y)\n",
    "\n",
    "        default_rule_ant = Antecedent({})\n",
    "        default_rule_conseq = Consequent(clf.default_class_attribute, clf.default_class)\n",
    "\n",
    "        clf.default_rule = ClassAssocationRule(\n",
    "            default_rule_ant,\n",
    "            default_rule_conseq,\n",
    "            clf.default_class_support,\n",
    "            clf.default_class_confidence\n",
    "        )\n",
    "\n",
    "\n",
    "import collections\n",
    "import time\n",
    "import random\n",
    "\n",
    "class M1Algorithm(RuleBuilderAlgorithm):\n",
    "    \"\"\" M1 Algorithm implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        # list for storing rules to be used in the classifier\n",
    "        classifier = []\n",
    "        # list for storing default classes associated\n",
    "        # with rules in the classifier\n",
    "        default_classes = []\n",
    "        # list for storing errors of said default classes\n",
    "        default_classes_errors = []\n",
    "        # list for storing rule errors from classifier\n",
    "        rule_errors = []\n",
    "        # list for storing total errors\n",
    "        # (rule_errors + default_classes_errors)\n",
    "        total_errors = []\n",
    "        # class distribution\n",
    "        # for calculating the default's rule confidence\n",
    "        # and support\n",
    "        class_distribution = collections.Counter(self.y)\n",
    "        classdist_keys = list(class_distribution.keys())\n",
    "\n",
    "\n",
    "        # sorting rules based on the precedence operator\n",
    "        self.rules.sort(reverse=True)\n",
    "\n",
    "        # converting TransactionDB to a set\n",
    "        # so that set intersection and difference can be used\n",
    "        dataset = set(self.dataset)\n",
    "\n",
    "        # obtaining the set's length. We do this only once to\n",
    "        # save processing time.\n",
    "        # this is a constant variable\n",
    "        dataset_len = len(dataset)\n",
    "\n",
    "        # When we want to update the dataset_len, we use\n",
    "        # this variable. Length is updated by subtracting \n",
    "        # absolute support of a rule from it\n",
    "        dataset_len_updated = dataset_len\n",
    "        \n",
    "        \n",
    "\n",
    "        for rule in self.rules:\n",
    "            # if all data cases have been covered\n",
    "            # break the loop to save time\n",
    "            if (dataset_len_updated <= 0):\n",
    "                break\n",
    "            \n",
    "\n",
    "            # temp serves for storing datacases\n",
    "            # that have been covered by current rule\n",
    "            temp = set()\n",
    "            # temp len is for determining temp's length\n",
    "            # without using len(temp) to save time\n",
    "            temp_len = 0\n",
    "            # number of rule that satisfy both antecedent\n",
    "            # and consequent of the current rule\n",
    "            temp_satisfies_conseq_cnt = 0\n",
    "            \n",
    "            \n",
    "            for datacase in dataset:\n",
    "                # if datacase satisfies rule's antecedent\n",
    "                # we'll store it in temp and increment\n",
    "                #  temp's len\n",
    "                if rule.antecedent <= datacase:\n",
    "                    temp.add(datacase)\n",
    "                    temp_len += 1\n",
    "\n",
    "                    # we'll mark the rule if datacase\n",
    "                    # satisfies its consequent. And increment\n",
    "                    # the counter\n",
    "                    if rule.consequent == datacase.class_val:  \n",
    "                        temp_satisfies_conseq_cnt += 1\n",
    "                        rule.marked = True\n",
    "                        \n",
    "\n",
    "            # if rule satisfied at least one consequent\n",
    "            if rule.marked:\n",
    "                classifier.append(rule)\n",
    "\n",
    "                # we subtract already covered rules\n",
    "                # from dataset                \n",
    "                dataset -= temp\n",
    "                # and update dataset's length\n",
    "                dataset_len_updated -= temp_len\n",
    "                \n",
    "                # we'll obtain Counter of remaining class values\n",
    "                # in the dataset using map to save time\n",
    "                class_distribution = collections.Counter(map(lambda d: d.class_val.value, dataset))\n",
    "                \n",
    "                # the most common value from the counter will be\n",
    "                # the default class\n",
    "                most_common_tuple = class_distribution.most_common(1)\n",
    "                \n",
    "\n",
    "                # here we'll do some checking in case\n",
    "                # the counter is empty\n",
    "                most_common_cnt = 0\n",
    "                most_common_label = \"None\"\n",
    "                \n",
    "                try:\n",
    "                    most_common_tuple = most_common_tuple[0]\n",
    "                    most_common_cnt = most_common_tuple[1]\n",
    "                    most_common_label = most_common_tuple[0]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                \n",
    "                    \n",
    "                # the most common label will be inserted into \n",
    "                # the list                \n",
    "                default_classes.append(most_common_label)\n",
    "                \n",
    "                \n",
    "                # number of errors the rule will make => \n",
    "                #\n",
    "                # difference of:\n",
    "                # all transactions that satisfy its antecedent\n",
    "                # and\n",
    "                # all transactions that satisfy both antecedent and consequent\n",
    "                rule_errors.append(temp_len - temp_satisfies_conseq_cnt)\n",
    "                \n",
    "                # default errors\n",
    "                #\n",
    "                # difference of:\n",
    "                # length of remaining dataset\n",
    "                # and\n",
    "                # count of most common class \n",
    "                dflt_class_err = dataset_len_updated - most_common_cnt\n",
    "                \n",
    "                \n",
    "                err_cnt = dflt_class_err\n",
    "                    \n",
    "                \n",
    "                default_classes_errors.append(err_cnt)\n",
    "                \n",
    "                total_errors.append(err_cnt + sum(rule_errors))\n",
    "                \n",
    "                \n",
    "        \n",
    "        # finding the smallest number of errors\n",
    "        # but checking if at least one rule classified an instance\n",
    "        if len(total_errors) != 0:            \n",
    "            min_errors = min(total_errors)\n",
    "            \n",
    "            # finding the index of smallest number of errors\n",
    "            idx_to_cut = total_errors.index(min_errors)\n",
    "            \n",
    "            final_classifier = classifier[:idx_to_cut+1]\n",
    "            default_class = default_classes[idx_to_cut]        \n",
    "            \n",
    "            # creating the final classifier\n",
    "            clf = Classifier()\n",
    "            clf.rules = final_classifier\n",
    "            clf.default_class = default_class\n",
    "            clf.default_class_attribute = classdist_keys[0][0]\n",
    "\n",
    "        else:\n",
    "            clf = Classifier()\n",
    "            clf.rules = []\n",
    "\n",
    "            possible_default_classes = list(class_distribution)\n",
    "            random_class_idx = random.randrange(0, len(possible_default_classes))\n",
    "            default_class_att, default_class_value = classdist_keys[random_class_idx]\n",
    "            clf.default_class = default_class_value\n",
    "            clf.default_class_attribute = default_class_att\n",
    "\n",
    "\n",
    "        self.calculate_default_class_properties(clf)        \n",
    "\n",
    "        return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import collections\n",
    "\n",
    "class M2Algorithm(RuleBuilderAlgorithm):\n",
    "    \"\"\"\n",
    "    Implementation of M2 Algorithm for CBA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build(self):\n",
    "\n",
    "        self.rules.sort(reverse=True)\n",
    "        \n",
    "        self.dataset_frozen = self.dataset\n",
    "        self.dataset_len = len(self.dataset_frozen)\n",
    "\n",
    "        # set of crules that have higher precedence\n",
    "        # that their corresponding wrules\n",
    "        self.Q = set()\n",
    "        \n",
    "        # set of all crules\n",
    "        self.U = set()\n",
    "        \n",
    "        # set of conflicting rules\n",
    "        self.A = set()\n",
    "        \n",
    "        self.classifier = []\n",
    "        \n",
    "        self.stage1()\n",
    "        self.stage2()\n",
    "        self.stage3()\n",
    "        \n",
    "        clf = Classifier()\n",
    "        clf.rules = self.classifier\n",
    "        clf.default_class = self.default_class\n",
    "        self.calculate_default_class_properties(clf)\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "        \n",
    "    def stage1(self):\n",
    "        for datacase in self.dataset_frozen:\n",
    "            # finds the highest precedence crules and wrules\n",
    "            crule, wrule = self.maxcoverrule(datacase, self.rules)\n",
    "        \n",
    "            if crule is None:\n",
    "                crule = self.emptyrule()\n",
    "                \n",
    "            if wrule is None:\n",
    "                wrule = self.emptyrule()\n",
    "                \n",
    "            self.U.add(crule)\n",
    "            \n",
    "            crule.class_cases_covered.update([datacase.class_val.value])\n",
    "            \n",
    "            if crule > wrule:\n",
    "                self.Q.add(crule)\n",
    "                crule.marked = True\n",
    "            else:\n",
    "                structure = (datacase, datacase.class_val.value, crule, wrule)\n",
    "                self.A.add(structure)\n",
    "                \n",
    "            \n",
    "                \n",
    "    \n",
    "    def stage2(self):\n",
    "        \n",
    "        for conflicting_struct in self.A:\n",
    "            datacase, clazz, crule, wrule = conflicting_struct\n",
    "            \n",
    "            \n",
    "            if wrule.marked:\n",
    "                crule.class_cases_covered[clazz] -= 1\n",
    "                wrule.class_cases_covered[clazz] += 1\n",
    "            \n",
    "            else:\n",
    "                wset = self.allcover_rules(self.U, datacase, crule)\n",
    "                for w in wset:\n",
    "                    w.replace.add((crule, datacase, clazz))\n",
    "                    w.class_cases_covered[clazz] += 1\n",
    "                    \n",
    "                self.Q = self.Q.union(wset)\n",
    "        \n",
    "        \n",
    "    def stage3(self):\n",
    "        Qlist = sorted(self.Q, reverse=True)\n",
    "\n",
    "        rule_errors = 0\n",
    "        rule_supcount = 0\n",
    "        total_errors_list = []\n",
    "        default_classes_list = []\n",
    "        rules_list = []\n",
    "        \n",
    "        # class distribution\n",
    "        classdist = collections.Counter(map(lambda d: d.class_val.value, self.dataset_frozen))\n",
    "        classdist_keys = list(classdist.keys())\n",
    "        \n",
    "        for rule in Qlist:\n",
    "            if rule.class_cases_covered[rule.consequent.value] > 0:\n",
    "                for (rule_replace, dcase, clazz) in rule.replace:\n",
    "                    if dcase.alreadycovered == True:\n",
    "                        rule.class_cases_covered[clazz] -= 1\n",
    "                    else:\n",
    "                        dcase.alreadycovered = True\n",
    "                        rule_replace.class_cases_covered[clazz] -= 1\n",
    "                \n",
    "                rule_errors += self.errors_of_rule(rule)\n",
    "                rule_supcount += rule.support_count\n",
    "                \n",
    "                classdist = self.update_class_distr(classdist, rule)\n",
    "                \n",
    "                default_class = self.select_default_class(classdist)\n",
    "                default_class_count = default_class[1]\n",
    "                default_class_label = default_class[0]\n",
    "                \n",
    "                default_errors = self.dataset_len - rule_supcount - default_class_count\n",
    "                \n",
    "                total_errors = rule_errors + default_errors\n",
    "                \n",
    "                rules_list.append(rule)\n",
    "                default_classes_list.append(default_class_label)\n",
    "                total_errors_list.append(total_errors)\n",
    "                \n",
    "        \n",
    "        if len(total_errors_list) != 0:\n",
    "            min_value = min(total_errors_list)\n",
    "            \n",
    "            min_indices = [ idx for (idx, err_num) in enumerate(total_errors_list) if err_num == min_value ]\n",
    "            min_idx = min_indices[0]\n",
    "            \n",
    "            final_classifier = [ rule for rule in rules_list[:min_idx + 1] ]\n",
    "            default_class = default_classes_list[min_idx]\n",
    "\n",
    "            if not default_class:\n",
    "                i = min_idx\n",
    "                while not default_class:\n",
    "                    i -= 1\n",
    "                    default_class = default_classes_list[i]\n",
    "\n",
    "            self.classifier = final_classifier\n",
    "            self.default_class = default_class\n",
    "            self.default_class_attribute = classdist_keys[0][0]\n",
    "        else:\n",
    "            possible_default_classes = list(classdist)\n",
    "            random_class_idx = random.randrange(0, len(possible_default_classes))\n",
    "            default_class_att, default_class_value = list(classdist.keys())[random_class_idx]\n",
    "\n",
    "            self.classifier = []\n",
    "            self.default_class = default_class_value\n",
    "            self.default_class_attribute = default_class_att\n",
    "    \n",
    "    def emptyrule(self):\n",
    "        \"\"\"returns rule with empty antecedent\n",
    "        and consequent\n",
    "        \"\"\"\n",
    "        return ClassAssocationRule(Antecedent([]), Consequent(None, None), 0, 0)\n",
    "    \n",
    "    \n",
    "    def maxcoverrule(self, datacase, rules):\n",
    "        \"\"\"\n",
    "        finds the highest precedence rule that covers\n",
    "        the case d\n",
    "        \n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        rules: sorted rules\n",
    "            \n",
    "        datacase: instance d\n",
    "            \n",
    "        \"\"\"\n",
    "        crule, wrule = None, None\n",
    "        \n",
    "        \n",
    "        for rule in rules:\n",
    "            if rule.antecedent <= datacase:\n",
    "                if rule.consequent == datacase.class_val and not crule:\n",
    "                    # save cRule\n",
    "                    crule = rule\n",
    "                    if crule and wrule:\n",
    "                        return crule, wrule\n",
    "                elif rule.consequent != datacase.class_val and not wrule:\n",
    "                    # save wRule\n",
    "                    wrule = rule\n",
    "                    if crule and wrule:\n",
    "                        return crule, wrule\n",
    "\n",
    "        \n",
    "        \n",
    "        return crule, wrule\n",
    "    \n",
    "    \n",
    "    def allcover_rules(self, U, datacase, crule):\n",
    "        \"\"\"method for finding all rules from a set U\n",
    "        that cover datacase and have a higher precedence\n",
    "        tha crule\n",
    "        \"\"\"\n",
    "        wset = set()\n",
    "        \n",
    "        for replacingrule in U:\n",
    "            if replacingrule > crule and replacingrule.antecedent <= datacase and replacingrule.consequent.value != datacase.class_val.value:\n",
    "                wset.add(replacingrule)\n",
    "        \n",
    "        return wset\n",
    "    \n",
    "    def errors_of_rule(self, rule):\n",
    "        \"\"\"method for computing errors of\n",
    "        a rule\n",
    "        \"\"\"\n",
    "        rule.support_count = sum(rule.class_cases_covered.values()) \n",
    "        return rule.support_count - rule.class_cases_covered[rule.consequent.value]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def select_default_class(self, classdist):\n",
    "        \"\"\"method for selecting default class\n",
    "        from class distribution\n",
    "        \"\"\"\n",
    "        most_common = classdist.most_common(1)\n",
    "        \n",
    "        if not most_common:\n",
    "            return (None, 0)\n",
    "        \n",
    "        return most_common[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import fim\n",
    "import logging\n",
    "\n",
    "def createCARs(rules):\n",
    "    \"\"\"Function for converting output from fim.arules or fim.apriori\n",
    "    to a list of ClassAssociationRules\n",
    "    Parameters\n",
    "    ----------\n",
    "    rules : output from fim.arules or from generateCARs\n",
    "    Returns\n",
    "    -------\n",
    "    list of CARs\n",
    "    \"\"\"\n",
    "    CARs = []\n",
    "    \n",
    "    for rule in rules:\n",
    "        con_tmp, ant_tmp, support, confidence = rule\n",
    "\n",
    "        con = Consequent(*con_tmp.split(\":=:\"))\n",
    "\n",
    "        # so that the order of items in antecedent is always the same\n",
    "        ant_tmp = sorted(list(ant_tmp))\n",
    "        ant_items = [ Item(*i.split(\":=:\")) for i in ant_tmp ]\n",
    "        ant = Antecedent(ant_items)\n",
    "\n",
    "        CAR = ClassAssocationRule(ant, con, support=support, confidence=confidence)\n",
    "        CARs.append(CAR)\n",
    "\n",
    "    CARs.sort(reverse=True)\n",
    "\n",
    "    return CARs\n",
    "\n",
    "\n",
    "def generateCARs(transactionDB, support, confidence, maxlen=10, **kwargs):\n",
    "    \"\"\"Function for generating ClassAssociationRules from a TransactionDB\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactionDB : TransactionDB\n",
    "    support : float\n",
    "        minimum support in percents if positive\n",
    "        absolute minimum support if negative\n",
    "    confidence : float\n",
    "        minimum confidence in percents if positive\n",
    "        absolute minimum confidence if negative\n",
    "    maxlen : int\n",
    "        maximum length of mined rules\n",
    "    **kwargs : \n",
    "        arbitrary number of arguments that will be \n",
    "        provided to the fim.apriori function\n",
    "    Returns\n",
    "    -------\n",
    "    list of CARs\n",
    "    \"\"\"\n",
    "    appear = transactionDB.appeardict\n",
    "    \n",
    "    rules = fim.apriori(transactionDB.string_representation, supp=support, conf=confidence, mode=\"o\", target=\"r\", report=\"sc\", appear=appear, **kwargs, zmax=maxlen)\n",
    "    \n",
    "\n",
    "    return createCARs(rules)\n",
    " \n",
    "\n",
    "def top_rules(transactions,\n",
    "              appearance={},\n",
    "              target_rule_count=1000,\n",
    "              init_support=0.5,\n",
    "              init_conf=0.5,\n",
    "              conf_step=0.05,\n",
    "              supp_step=0.05,\n",
    "              minlen=2,\n",
    "              init_maxlen=3,\n",
    "              total_timeout=100.,\n",
    "              max_iterations=30):\n",
    "    \"\"\"Function for finding the best n (target_rule_count)\n",
    "    rules from transaction list\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions : 2D array of strings\n",
    "        e.g. [[\"a:=:1\", \"b:=:3\"], [\"a:=:4\", \"b:=:2\"]]\n",
    "    appearance : dictionary\n",
    "        dictionary specifying rule appearance\n",
    "    targent_rule_count : int\n",
    "        target number of rules to mine\n",
    "    init_conf : float\n",
    "        confidence from which to start mining\n",
    "    conf_step : float\n",
    "    supp_step : float\n",
    "    minen : int\n",
    "        minimum len of rules to mine\n",
    "    init_maxlen : int\n",
    "        maxlen from which to start mining\n",
    "    total_timeout : float\n",
    "        maximum execution time of the function\n",
    "    max_iterations : int\n",
    "        maximum iterations to try before stopping\n",
    "        execution\n",
    "    Returns\n",
    "    -------\n",
    "    list of mined rules. The rules are not ordered.\n",
    "    \"\"\"\n",
    "    \n",
    "    starttime = time.time()\n",
    "    \n",
    "    MAX_RULE_LEN = len(transactions[0])\n",
    "    \n",
    "    support = init_support\n",
    "    conf = init_conf\n",
    "    \n",
    "    maxlen = init_maxlen\n",
    "    \n",
    "    flag = True\n",
    "    lastrulecount = -1\n",
    "    maxlendecreased_due_timeout = False\n",
    "    iterations = 0\n",
    "    \n",
    "    rules = None\n",
    "\n",
    "    while flag:\n",
    "        iterations += 1\n",
    "            \n",
    "        if iterations == max_iterations:\n",
    "            logging.debug(\"Max iterations reached\")\n",
    "            break\n",
    "\n",
    "        logging.debug(\"Running apriori with setting: confidence={}, support={}, minlen={}, maxlen={}, MAX_RULE_LEN={}\".format(\n",
    "                conf, support, minlen, maxlen, MAX_RULE_LEN))\n",
    "        \n",
    "        rules_current = fim.arules(transactions, supp=support, conf=conf, mode=\"o\", report=\"sc\", appear=appearance, zmax=maxlen, zmin=minlen)\n",
    "        \n",
    "        rules = rules_current\n",
    "        \n",
    "        rule_count = len(rules)\n",
    "        \n",
    "        logging.debug(\"Rule count: {}, Iteration: {}\".format(rule_count, iterations))\n",
    "        \n",
    "        if (rule_count >= target_rule_count):\n",
    "            flag = False\n",
    "            logging.debug(f\"Target rule count satisfied: {target_rule_count}\")\n",
    "        else:\n",
    "            exectime = time.time() - starttime\n",
    "            \n",
    "            if exectime > total_timeout:\n",
    "                logging.debug(f\"Execution time exceeded: {total_timeout}\")\n",
    "                flag = False\n",
    "                \n",
    "            elif maxlen < MAX_RULE_LEN and lastrulecount != rule_count and not maxlendecreased_due_timeout:\n",
    "                    maxlen += 1\n",
    "                    lastrulecount = rule_count\n",
    "                    logging.debug(f\"Increasing maxlen {maxlen}\")\n",
    "                        \n",
    "            elif maxlen < MAX_RULE_LEN and maxlendecreased_due_timeout and support <= 1 - supp_step:\n",
    "                support += supp_step\n",
    "                maxlen += 1\n",
    "                lastrulecount = rule_count\n",
    "                \n",
    "                logging.debug(f\"Increasing maxlen to {maxlen}\")\n",
    "                logging.debug(f\"Increasing minsup to {support}\")\n",
    "                \n",
    "                maxlendecreased_due_timeout = False\n",
    "            \n",
    "            elif conf > conf_step:\n",
    "                conf -= conf_step\n",
    "                logging.debug(f\"Decreasing confidence to {conf}\")\n",
    "                \n",
    "            else:\n",
    "                logging.debug(\"All options exhausted\")\n",
    "                flag = False\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "class Antecedent(ComparableItemSet):\n",
    "    \"\"\"Antecedent represents a left-hand side of the association rule.\n",
    "    It is a set of conditions (Items) a Transaction has to satisfy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    items: 1D array of Items\n",
    "    Attributes\n",
    "    ----------\n",
    "    itemset: 1D array of Items\n",
    "        dictionary of unique attributes, such as: {a: 1, b: 3}\n",
    "    frozenset: frozenset of Items\n",
    "        this attribute is vital for determining if antecedent\n",
    "        is a subset of transaction and, consequently, if transaction\n",
    "        satisfies antecedent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, items):\n",
    "\n",
    "        # extract unique attributes and convert them to dict\n",
    "        # such as: {a: 1, b: 3, c: 4}\n",
    "        self.itemset = dict(list(set(items)))\n",
    "\n",
    "        # this part is important for better performance\n",
    "        # of M1 and M2 algoritms\n",
    "        self.frozenset = frozenset(self)\n",
    "        \n",
    "    \n",
    "    def __getattr__(self, attr_name):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        attribute: str\n",
    "            name of desired attribute\n",
    "        Returns\n",
    "        -------\n",
    "        Attribute of given name, otherwise an AttributeError\n",
    "        \"\"\"\n",
    "        item = self.itemset.get(attr_name, None)\n",
    "        \n",
    "        if (item):\n",
    "            return item\n",
    "        else:\n",
    "            raise AttributeError(\"No attribute of that name\")\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Method which allows indexing on antecedent's itemset\n",
    "        \"\"\"\n",
    "        items = list(self.itemset.items())\n",
    "        \n",
    "        if (idx <= len(items)):\n",
    "            return items[idx]\n",
    "        else:\n",
    "            raise IndexError(\"No value at the specified index\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        length of the itemset\n",
    "        \"\"\"\n",
    "        return len(self.itemset)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        str_array = [repr((attr, val)) for attr, val in self.itemset.items()]\n",
    "        text = \", \".join(str_array)\n",
    "        return \"Antecedent({})\".format(text)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.itemset.items()))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "\n",
    "    def string(self):\n",
    "        items = list(self.itemset.items())\n",
    "        string_items = [ \"{}={}\".format(key, val) for key, val in items ]\n",
    "\n",
    "        string_ant = \",\".join(string_items)\n",
    "\n",
    "        return \"{\" + string_ant + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Consequent(Item, ComparableItemSet):\n",
    "    \"\"\"\n",
    "    Represents a right-hand side of the association rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    def getclass(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        item_tuple = self.attribute, self.value\n",
    "        return \"Consequent{{{}}}\".format(item_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBA():\n",
    "    \"\"\"Class for training a testing the\n",
    "    CBA Algorithm.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    support : float\n",
    "    confidence : float\n",
    "    algorithm : string\n",
    "        Algorithm for building a classifier.\n",
    "    maxlen : int\n",
    "        maximum length of mined rules\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, support=0.10, confidence=0.5, maxlen=10, algorithm=\"m1\"):\n",
    "        if algorithm not in [\"m1\", \"m2\"]:\n",
    "            raise Exception(\"algorithm parameter must be either 'm1' or 'm2'\")\n",
    "        if 0 > support or support > 1:\n",
    "            raise Exception(\"support must be on the interval <0;1>\")\n",
    "        if 0 > confidence or confidence > 1:\n",
    "            raise Exception(\"confidence must be on the interval <0;1>\")\n",
    "        if maxlen < 1:\n",
    "            raise Exception(\"maxlen cannot be negative or 0\")\n",
    "\n",
    "        self.support = support * 100\n",
    "        self.confidence = confidence * 100\n",
    "        self.algorithm = algorithm\n",
    "        self.maxlen = maxlen\n",
    "        self.clf = None\n",
    "        self.target_class = None\n",
    "\n",
    "        self.available_algorithms = {\n",
    "            \"m1\": M1Algorithm,\n",
    "            \"m2\": M2Algorithm\n",
    "        }\n",
    "\n",
    "    def rule_model_accuracy(self, txns):\n",
    "        \"\"\"Takes a TransactionDB and outputs\n",
    "        accuracy of the classifier\n",
    "        \"\"\"\n",
    "        if not self.clf:\n",
    "            raise Exception(\"CBA must be trained using fit method first\")\n",
    "        if not isinstance(txns, TransactionDB):\n",
    "            raise Exception(\"txns must be of type TransactionDB\")\n",
    "\n",
    "        return self.clf.test_transactions(txns)\n",
    "\n",
    "    def fit(self, transactions, top_rules_args={}):\n",
    "        \"\"\"Trains the model based on input transaction\n",
    "        and returns self.\n",
    "        \"\"\"\n",
    "        if not isinstance(transactions, TransactionDB):\n",
    "            raise Exception(\"transactions must be of type TransactionDB\")\n",
    "\n",
    "        self.target_class = transactions.header[-1]\n",
    "\n",
    "        used_algorithm = self.available_algorithms[self.algorithm]\n",
    "\n",
    "        cars = None\n",
    "\n",
    "        if not top_rules_args:\n",
    "            cars = generateCARs(transactions, support=self.support, confidence=self.confidence, maxlen=self.maxlen)\n",
    "        else:\n",
    "            rules = top_rules(transactions.string_representation, appearance=transactions.appeardict, **top_rules_args)\n",
    "            cars = createCARs(rules)\n",
    "\n",
    "        self.clf = used_algorithm(cars, transactions).build()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Method that can be used for predicting\n",
    "        classes of unseen cases.\n",
    "        CBA.fit must be used before predicting.\n",
    "        \"\"\"\n",
    "        if not self.clf:\n",
    "            raise Exception(\"CBA must be train using fit method first\")\n",
    "\n",
    "        if not isinstance(X, TransactionDB):\n",
    "            raise Exception(\"X must be of type TransactionDB\")\n",
    "\n",
    "        return self.clf.predict_all(X)\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        \"\"\"Method for predicting probablity of\n",
    "        given classification\n",
    "\n",
    "        CBA.fit must be used before predicting probablity.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.clf.predict_probability_all(X)\n",
    "\n",
    "    def predict_matched_rules(self, X):\n",
    "        \"\"\"for each data instance, returns a rule that\n",
    "        matched it according to the CBA order (sorted by\n",
    "        confidence, support and length)\n",
    "        \"\"\"\n",
    "\n",
    "        return self.clf.predict_matched_rule_all(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruning():\n",
    "    def prune(cars, min_support): \n",
    "        index = 0\n",
    "        supportList = []\n",
    "        prunedRules = []\n",
    "        while True:\n",
    "            support = cars[index]\n",
    "            support = str(support)\n",
    "            support = support.split('sup: ', 1)[1]\n",
    "            support = support.split(' ')\n",
    "            support = support[0]\n",
    "            if float(support) >= float(min_support):\n",
    "                supportList.append(support)\n",
    "                prunedRules.append(cars[index])\n",
    "            index = index + 1\n",
    "            if index == len(cars):\n",
    "                break\n",
    "        return prunedRules\n",
    "    \n",
    "    def prunes(cars):\n",
    "        candidateList = []\n",
    "        candidateList = generateCandidates(cars)\n",
    "    \n",
    "    def generateCandidates(cars):\n",
    "        LHS = []\n",
    "        index = 0\n",
    "        while True:\n",
    "            LHS = str(cars[1])\n",
    "            LHS = LHS.split('CAR {',1)[1]\n",
    "            LHS = LHS.split(' ')\n",
    "            LHS = LHS[0]\n",
    "            LHS = LHS[:-1]\n",
    "            LHS = LHS.split(',')\n",
    "            \n",
    "            RHS = str(cars[index])\n",
    "            RHS = RHS.split('=> ',1)[1]\n",
    "            RHS = RHS.split(' ')\n",
    "            RHS = RHS[0]\n",
    "            RHS = RHS[1:-1]\n",
    "            RHS = RHS.split()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleExtender:\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "    \n",
    "        if type(dataframe) != QuantitativeDataFrame:\n",
    "            raise Exception(\n",
    "                \"type of dataset must be pandas.DataFrame\"\n",
    "            )\n",
    "            \n",
    "        self.__dataframe = dataframe\n",
    "        \n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        progress_bar_len = 50\n",
    "        copied_rules_len = len(copied_rules)\n",
    "        progress_bar = \"#\" * progress_bar_len\n",
    "        progress_bar_empty = \" \" * progress_bar_len\n",
    "        last_progress_bar_idx = -1\n",
    "\n",
    "        extended_rules = []\n",
    "\n",
    "        #print(\"len: \", copied_rules_len)\n",
    "\n",
    "        for i, rule in enumerate(copied_rules):\n",
    "            current_progress_bar_idx = math.floor(i / copied_rules_len * progress_bar_len)\n",
    "            \n",
    "            if last_progress_bar_idx != current_progress_bar_idx:\n",
    "                last_progress_bar_idx = current_progress_bar_idx\n",
    "                \n",
    "                progress_string = \"[\" + progress_bar[:last_progress_bar_idx] + progress_bar_empty[last_progress_bar_idx:] + \"]\"\n",
    "                \n",
    "                print(*progress_string, sep=\"\")\n",
    "           # print(\"BEFORE1\")\n",
    "            extended_rules.append(self.__extend(rule))\n",
    "            #print(\"AFTER\")\n",
    "        \n",
    "        return extended_rules\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __extend(self, rule):\n",
    "        ext = self.__extend_rule(rule)\n",
    "        \n",
    "        return ext\n",
    "        \n",
    "    def __extend_rule(self, rule, min_improvement=-100, min_conditional_improvement=-0.01):\n",
    "        # check improvemnt argument ranges\n",
    "        current_best = rule\n",
    "        direct_extensions = self.__get_extensions(rule)\n",
    "        \n",
    "        current_best.update_properties(self.__dataframe)\n",
    "        \n",
    "        while True:\n",
    "            extension_succesful = True\n",
    "\n",
    "            direct_extensions = self.__get_extensions(current_best)\n",
    "\n",
    "            #print(\"extending - new cycle\")\n",
    "            \n",
    "            for candidate in direct_extensions:\n",
    "                #print(\"\\tcandidate - direct extensions\")\n",
    "                candidate.update_properties(self.__dataframe)\n",
    "                \n",
    "                delta_confidence = candidate.confidence - current_best.confidence\n",
    "                delta_support = candidate.support - current_best.support\n",
    "                \n",
    "                \n",
    "                if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                    current_best = candidate\n",
    "                    extension_succesful = True\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                if self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                    enlargement = candidate\n",
    "                    \n",
    "                    while True:\n",
    "                        enlargement = self.get_beam_extensions(enlargement)\n",
    "                        #print(\"LINE 1\")\n",
    "                        if enlargement:\n",
    "                            print(\"LINE 2\")\n",
    "                            break\n",
    "                            \n",
    "                        candidate.update_properties(self.__dataframe)\n",
    "                        enlargement.update_properties(self.__dataframe)\n",
    "\n",
    "                        delta_confidence = enlargement.confidence - current_best.confidence\n",
    "                        delta_support = enlargement.support - current_best.support\n",
    "                        print(delta_confidence)\n",
    "                        print(delta_support)\n",
    "                        if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                            print(\"LINE 3\")\n",
    "                            current_best = enlargement\n",
    "                            extension_succesful = True\n",
    "                            \n",
    "                        elif self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                            print(\"LINE 4\")\n",
    "                            break\n",
    "                        \n",
    "                        else:\n",
    "                            print(\"LINE 5\")\n",
    "                            break\n",
    "            \n",
    "            \n",
    "                    if extension_succesful == True:\n",
    "                        break\n",
    "                        \n",
    "                else:\n",
    "                    # continue to next candidate\n",
    "                    break\n",
    "           \n",
    "        \n",
    "            if extension_succesful == True:\n",
    "                break\n",
    "                    \n",
    "        #print('EXITING')\n",
    "        return current_best\n",
    "        \n",
    "        \n",
    "    def __get_extensions(self, rule):\n",
    "        extended_rules = []\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            neighborhood = self.__get_direct_extensions(literal)\n",
    "            \n",
    "            for extended_literal in neighborhood:\n",
    "                # copy the rule so the extended literal\n",
    "                # can replace the default literal\n",
    "                copied_rule = rule.copy()\n",
    "                \n",
    "                # find the index of the literal\n",
    "                # so that it can be replaced\n",
    "                current_literal_index = copied_rule.antecedent.index(literal)\n",
    "                \n",
    "                copied_rule.antecedent[current_literal_index] = extended_literal\n",
    "                copied_rule.was_extended = True\n",
    "                copied_rule.extended_literal = extended_literal\n",
    "                \n",
    "                extended_rules.append(copied_rule)\n",
    "\n",
    "        extended_rules.sort(reverse=True)\n",
    "             \n",
    "        return extended_rules\n",
    "            \n",
    "    \n",
    "    def __get_direct_extensions(self, literal):\n",
    "        \"\"\"\n",
    "        ensure sort and unique\n",
    "        before calling functions\n",
    "        \"\"\"\n",
    "        \n",
    "        attribute, interval = literal\n",
    "\n",
    "        # if nominal\n",
    "        # needs correction to return null and skip when extending\n",
    "        if type(interval) == str:\n",
    "            return [literal]\n",
    "        \n",
    "        vals = self.__dataframe.column(attribute)\n",
    "        vals_len = vals.size\n",
    "\n",
    "        mask = interval.test_membership(vals)\n",
    "\n",
    "        # indices of interval members\n",
    "        # we want to extend them \n",
    "        # once to the left\n",
    "        # and once to the right\n",
    "        # bu we have to check if resulting\n",
    "        # indices are not larger than value size\n",
    "        member_indexes = np.where(mask)[0]\n",
    "\n",
    "        first_index = member_indexes[0]\n",
    "        last_index = member_indexes[-1]\n",
    "\n",
    "        first_index_modified = first_index - 1\n",
    "        last_index_modified = last_index + 1\n",
    "        \n",
    "        no_left_extension = False\n",
    "        no_right_extension = False\n",
    "\n",
    "        if first_index_modified < 0:\n",
    "            no_left_extension = True\n",
    "\n",
    "        # if last_index_modified is larger than\n",
    "        # available indices\n",
    "        if last_index_modified > vals_len - 1:\n",
    "            no_right_extension = True\n",
    "\n",
    "\n",
    "        new_left_bound = interval.minval\n",
    "        new_right_bound = interval.maxval\n",
    "\n",
    "        if not no_left_extension:\n",
    "            new_left_bound = vals[first_index_modified]\n",
    "\n",
    "        if not no_right_extension:\n",
    "            new_right_bound = vals[last_index_modified]\n",
    "\n",
    "\n",
    "        # prepare return values\n",
    "        extensions = []\n",
    "\n",
    "        if not no_left_extension:\n",
    "            # when values are [1, 2, 3, 3, 4, 5]\n",
    "            # and the corresponding interval is (2, 4)\n",
    "            # instead of resulting interval being (1, 4)\n",
    "            \n",
    "            temp_interval = Interval(\n",
    "                new_left_bound,\n",
    "                interval.maxval,\n",
    "                True,\n",
    "                interval.right_inclusive\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        if not no_right_extension:\n",
    "\n",
    "            temp_interval = Interval(\n",
    "                interval.minval,\n",
    "                new_right_bound,\n",
    "                interval.left_inclusive,\n",
    "                True\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        return extensions\n",
    "        \n",
    "    \n",
    "    # make private\n",
    "    def get_beam_extensions(self, rule):\n",
    "        if not rule.was_extended:\n",
    "            return None\n",
    "\n",
    "        # literal which extended the rule\n",
    "        literal = rule.extended_literal\n",
    "        \n",
    "        extended_literal = self.__get_direct_extensions(literal)\n",
    "        \n",
    "        if not extended_literal:\n",
    "            return None\n",
    "        \n",
    "        copied_rule = rule.copy()\n",
    "        \n",
    "        literal_index = copied_rule.antecedent.index(literal)\n",
    "        \n",
    "        # so that literal is not an array\n",
    "        copied_rule.antecedent[literal_index] = extended_literal[0]\n",
    "        copied_rule.was_extended = True\n",
    "        copied_rule.extended_literal = extended_literal[0]\n",
    "        #print(copied_rule)\n",
    "        return copied_rule\n",
    "\n",
    "    \n",
    "    \n",
    "    def __crisp_accept(self, delta_confidence, delta_support, min_improvement):\n",
    "        if delta_confidence >= min_improvement and delta_support >= 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __conditional_accept(self, delta_conf, min_improvement):\n",
    "        if delta_conf >= min_improvement:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleOverlapPruner:\n",
    "    \n",
    "    def __init__(self, quantitative_dataset):\n",
    "        self.__dataframe = quantitative_dataset\n",
    "        \n",
    "        \n",
    "    def transform(self, rules, default_class, transaction_based=True):\n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        pruned_rules = copied_rules\n",
    "\n",
    "        if transaction_based:\n",
    "            pruned_rules = self.prune_transaction_based(copied_rules, default_class)        \n",
    "        else:\n",
    "            pruned_rules = self.prune_range_based(copied_rules, default_class)\n",
    "\n",
    "        return pruned_rules\n",
    "    \n",
    "    def prune_transaction_based(self, rules, default_class):\n",
    "        \"\"\"Transaction based\n",
    "        \"\"\"\n",
    "        \n",
    "        new_rules = [ rule for rule in rules ]\n",
    "        \n",
    "        for idx, rule in enumerate(rules):\n",
    "            \n",
    "            rule_classname, rule_classval = rule.consequent\n",
    "            \n",
    "            if rule_classval != default_class:\n",
    "                continue\n",
    "\n",
    "            correctly_covered_antecedent, correctly_covered_consequent = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "            correctly_covered = correctly_covered_antecedent & correctly_covered_consequent\n",
    "\n",
    "            non_empty_intersection = False\n",
    "            \n",
    "            for candidate_clash in rules[idx:]:\n",
    "                \n",
    "                cand_classname, cand_classval = candidate_clash.consequent\n",
    "                \n",
    "                if cand_classval == default_class:\n",
    "                    continue\n",
    "                    \n",
    "                cand_clash_covered_antecedent, cand_clash_covered_consequent = self.__dataframe.find_covered_by_rule_mask(candidate_clash)\n",
    "                \n",
    "                \n",
    "                if any(cand_clash_covered_antecedent & correctly_covered):\n",
    "                    non_empty_intersection = True\n",
    "                    break\n",
    "                    \n",
    "            if non_empty_intersection == False:\n",
    "                new_rules.remove(rule)\n",
    "                \n",
    "            \n",
    "        return new_rules\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def prune_range_based(self, rules, default_class):\n",
    "        \n",
    "        \"\"\"Transaction based\n",
    "        \"\"\"\n",
    "        \n",
    "        new_rules = [ rule for rule in rules ]\n",
    "        \n",
    "        for idx, rule in enumerate(rules):\n",
    "            \n",
    "            rule_classname, rule_classval = rule.consequent\n",
    "            \n",
    "            if rule_classval != default_class:\n",
    "                continue\n",
    "                            \n",
    "            literals = dict(rule.antecedent)\n",
    "            attributes = literals.keys()\n",
    "\n",
    "            clashing_rule_found = False\n",
    "            \n",
    "            \"\"\"\n",
    "            correctly_covered_antecedent, correctly_covered_consequent = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "            correctly_covered = correctly_covered_antecedent & correctly_covered_consequent\n",
    "            \"\"\"\n",
    "            non_empty_intersection = False\n",
    "            \n",
    "            \n",
    "            for candidate_clash in rules[idx:]:\n",
    "                \n",
    "                cand_classname, cand_classval = candidate_clash.consequent\n",
    "                \n",
    "                if cand_classval == default_class:\n",
    "                    continue\n",
    "                    \n",
    "                attributes_candclash = dict(candidate_clash.antecedent).keys()\n",
    "                shared_attributes = set(attributes) & set(attributes_candclash)\n",
    "                \n",
    "                if not shared_attributes:\n",
    "                    clashing_rule_found = True\n",
    "                    break\n",
    "                    \n",
    "                clash_cand_antecedent_dict = dict(candidate_clash.antecedent)\n",
    "                literals_in_clash_shared_att = [ (key, clash_cand_antecedent_dict[key]) for key in shared_attributes  ]\n",
    "                \n",
    "                at_least_one_attribute_disjunct = False\n",
    "                \n",
    "                for literal in literals_in_clash_shared_att:\n",
    "                    attribute, interval = literal\n",
    "                    \n",
    "                    temp_literal = attribute, literals[attribute]\n",
    "\n",
    "                    if not interval.overlaps_with(temp_literal[1]):\n",
    "                        at_least_one_attribute_disjunct = True\n",
    "                        break\n",
    "\n",
    "                    \n",
    "                if at_least_one_attribute_disjunct == False:\n",
    "                    clashing_rule_found == True\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            if clashing_rule_found == False:\n",
    "                new_rules.remove(rule)\n",
    "                \n",
    "            \n",
    "        return new_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RulePostPruner:\n",
    "    \n",
    "    def __init__(self, quantitative_dataset):\n",
    "        self.__dataframe = quantitative_dataset\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        pruned_rules = self.prune(copied_rules)\n",
    "        \n",
    "        return pruned_rules\n",
    "        \n",
    "    def preprocess_dataframe(self):\n",
    "        return self.__dataframe.dataframe.index.values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_most_frequent_class(self):\n",
    "        \"\"\" \n",
    "        requires class column to be the last in dataframe\n",
    "        \n",
    "        gets the most frequent class from dataset\n",
    "        - naive implementation\n",
    "        \"\"\"\n",
    "        \n",
    "        index_counts, possible_classes = pd.factorize(self.__dataframe.dataframe.iloc[:, -1].values)\n",
    "        counts = np.bincount(index_counts)\n",
    "        counts_max = counts.max()\n",
    "        most_frequent_classes = possible_classes[counts == counts_max]\n",
    "        \n",
    "        # return only one\n",
    "        return most_frequent_classes[0], counts_max\n",
    "    \n",
    "    \n",
    "    def get_most_frequent_from_numpy(self, ndarray):\n",
    "        \"\"\"gets a mode from numpy array\n",
    "        \"\"\"\n",
    "        unique, pos = np.unique(ndarray, return_inverse=True) \n",
    "        counts = np.bincount(pos)                  \n",
    "        maxpos = counts.argmax()                      \n",
    "\n",
    "        return (unique[maxpos], counts[maxpos])\n",
    "        \n",
    "    \n",
    "    def find_covered(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def prune(self, rules):\n",
    "        \n",
    "        dataset = self.preprocess_dataframe()\n",
    "        dataset_len = dataset.size\n",
    "        # True if datacase is not covered yet\n",
    "        dataset_mask = [ True ] * dataset_len\n",
    "        \n",
    "        cutoff_rule = rules[-1]\n",
    "        cutoff_class, cutoff_class_count = self.get_most_frequent_class()\n",
    "        \n",
    "        default_class = cutoff_class\n",
    "\n",
    "        total_errors_without_default = 0\n",
    "        \n",
    "        lowest_total_error = dataset_len - cutoff_class_count\n",
    "        \n",
    "        # implement comparators\n",
    "        rules.sort(reverse=True)\n",
    "        \n",
    "        for rule in rules:\n",
    "            covered_antecedent, covered_consequent = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "\n",
    "            \n",
    "            # dataset -= covered_antecedent\n",
    "            #dataset_mask = dataset_mask & np.logical_not(covered_antecedent)\n",
    "\n",
    "            correctly_covered = covered_antecedent & covered_consequent\n",
    "            \n",
    "            #print(\"correctly covered from mask\", np.sum(correctly_covered & dataset_mask))\n",
    "            \n",
    "            if not any(correctly_covered):\n",
    "                rules.remove(rule)\n",
    "            else:\n",
    "                misclassified = np.sum(covered_antecedent & dataset_mask) - np.sum(correctly_covered & dataset_mask)\n",
    "                \n",
    "                total_errors_without_default += misclassified\n",
    "                \n",
    "                # dataset -= covered_antecedent\n",
    "                #dataset_mask = np.logical_not(dataset_mask & covered_antecedent)\n",
    "                dataset_mask = dataset_mask & np.logical_not(covered_antecedent)\n",
    "\n",
    "\n",
    "                modified_dataset = dataset[dataset_mask]\n",
    "                class_values = self.__dataframe.dataframe.iloc[:,-1][dataset_mask].values\n",
    "\n",
    "                default_class, default_class_count = self.__dataframe.dataframe.iloc[1,-1], 0\n",
    "                \n",
    "                if len(class_values) > 0:\n",
    "                    default_class, default_class_count = self.get_most_frequent_from_numpy(class_values)\n",
    "                \n",
    "                # don't forget to update dataset length\n",
    "                default_rule_error = np.sum(dataset_mask) - default_class_count\n",
    "                total_errors_with_default = default_rule_error + total_errors_without_default\n",
    "                \n",
    "   \n",
    "                \n",
    "                if total_errors_with_default < lowest_total_error:\n",
    "                    cutoff_rule = rule\n",
    "                    lowest_total_error = total_errors_with_default\n",
    "                    cutoff_class = default_class\n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "        \n",
    "        # remove all rules below cutoff rule\n",
    "        index_to_cut = rules.index(cutoff_rule)\n",
    "        rules_pruned = rules[:index_to_cut+1]\n",
    "        \n",
    "        # append new default rule\n",
    "        empty_rule = cutoff_rule.copy()\n",
    "        empty_rule.antecedent = []\n",
    "        empty_rule.consequent = self.__dataframe.dataframe.columns[-1], cutoff_class\n",
    "        \n",
    "        \n",
    "        #rules_pruned.append(empty_rule)\n",
    "        \n",
    "        return rules_pruned, cutoff_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleLiteralPruner:\n",
    "    \n",
    "    def __init__(self, quantitative_dataframe):\n",
    "        self.__dataframe = quantitative_dataframe\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules  ]\n",
    "        trimmed = [ self.__trim(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return trimmed\n",
    "    \n",
    "    \n",
    "    def produce_combinations(self, array):\n",
    "        arr_len = len(array)\n",
    "    \n",
    "        for i in range(arr_len):\n",
    "            combination = array[0:i] + array[i+1:arr_len]\n",
    "        \n",
    "            yield combination\n",
    "    \n",
    "    \n",
    "    def __trim(self, rule):\n",
    "        \"\"\"\n",
    "        if type(rule) != QuantitativeCAR:\n",
    "            raise Exception(\"type of rule must be QuantClassAssociationRule\")\n",
    "        \"\"\"\n",
    "            \n",
    "        attr_removed = False\n",
    "    \n",
    "        literals = rule.antecedent\n",
    "        consequent = rule.consequent\n",
    "        \n",
    "        rule.update_properties(self.__dataframe)\n",
    "        \n",
    "        dataset_len = self.__dataframe.size\n",
    "\n",
    "        if len(literals) < 1:\n",
    "            return rule\n",
    "\n",
    "        while True:\n",
    "            for literals_combination in self.produce_combinations(literals):\n",
    "                if not literals_combination:\n",
    "                    continue\n",
    "                    \n",
    "                copied_rule = rule.copy()\n",
    "                \n",
    "                copied_rule.antecedent = literals_combination\n",
    "                copied_rule.update_properties(self.__dataframe)\n",
    "\n",
    "                if copied_rule.confidence > rule.confidence:\n",
    "                    rule.support = copied_rule.support\n",
    "                    rule.confidence = copied_rule.confidence\n",
    "                    rule.rulelen = copied_rule.rulelen\n",
    "                    \n",
    "                    rule.antecedent = copied_rule.antecedent\n",
    "\n",
    "                    attr_removed = True\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    attr_removed = False\n",
    "\n",
    "            if attr_removed == False:\n",
    "                break\n",
    "                \n",
    "                \n",
    "        return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleRefitter:\n",
    "    \"\"\"Refits the rule to a finer grid\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, quantitative_dataframe):\n",
    "        self.__dataframe = quantitative_dataframe\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules  ]\n",
    "        refitted = [ self.__refit(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return refitted\n",
    "        \n",
    "    def __refit(self, rule):\n",
    "        \"\"\"refits a single rule\n",
    "        \"\"\"\n",
    "\n",
    "        for idx, literal in enumerate(rule.antecedent):\n",
    "            attribute, interval = literal\n",
    "\n",
    "            # if nominal\n",
    "            if type(interval) == str:\n",
    "                continue\n",
    "        \n",
    "            current_attribute_values = self.__dataframe.column(attribute)\n",
    "\n",
    "            refitted_interval = interval.refit(current_attribute_values)\n",
    "\n",
    "            rule.antecedent[idx] = attribute, refitted_interval\n",
    "            \n",
    "            \n",
    "        return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleTrimmer:\n",
    "    \"\"\"Trims the rule\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, quantitative_dataframe):\n",
    "        self.__dataframe = quantitative_dataframe\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules  ]\n",
    "        trimmed = [ self.__trim(rule) for rule in copied_rules ]\n",
    "        \n",
    "        return trimmed\n",
    "    \n",
    "    \n",
    "    def __trim(self, rule):\n",
    "\n",
    "        \"\"\"\n",
    "        if type(rule) != QuantitativeCAR:\n",
    "            raise Exception(\"type of rule must be QuantClassAssociationRule\")\n",
    "        \"\"\"\n",
    "            \n",
    "        covered_by_antecedent_mask, covered_by_consequent_mask = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "        \n",
    "        covered_by_rule_mask = covered_by_antecedent_mask & covered_by_consequent_mask\n",
    "        \n",
    "        # instances covered by rule\n",
    "        correctly_covered_by_r = self.__dataframe.mask(covered_by_rule_mask)\n",
    "        \n",
    "        antecedent = rule.antecedent\n",
    "\n",
    "        for idx, literal in enumerate(antecedent):\n",
    "\n",
    "            attribute, interval = literal\n",
    "\n",
    "            # if nominal\n",
    "            if type(interval) == str:\n",
    "                continue\n",
    "            \n",
    "            current_column = correctly_covered_by_r[[attribute]].values\n",
    "            current_column_unique = np.unique(current_column)\n",
    "\n",
    "            if not current_column.any():\n",
    "                continue\n",
    "\n",
    "            minv = np.asscalar(min(current_column))\n",
    "            maxv = np.asscalar(max(current_column))\n",
    "\n",
    "            new_interval = Interval(minv, maxv, True, True)\n",
    "\n",
    "            antecedent[idx] = attribute, new_interval\n",
    "\n",
    "        return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval:\n",
    "\n",
    "    def __init__(self, minval, maxval, left_inclusive, right_inclusive):\n",
    "        self.minval = minval\n",
    "        self.maxval = maxval\n",
    "        self.left_inclusive = left_inclusive\n",
    "        self.right_inclusive = right_inclusive\n",
    "        \n",
    "        \n",
    "        self.left_bracket = \"<\" if left_inclusive else \"(\"\n",
    "        self.right_bracket = \">\" if right_inclusive else \")\"\n",
    "        \n",
    "        self.__membership_func = np.vectorize(\n",
    "            make_intervalfunc(self.minval, self.maxval, self.left_inclusive, self.right_inclusive)\n",
    "        )\n",
    "            \n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(repr(self))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "            \n",
    "    def refit(self, vals):\n",
    "        \"\"\"refit values to a finer grid\n",
    "        \"\"\"\n",
    "        values = np.array(vals)\n",
    "        \n",
    "        mask = self.test_membership(values)\n",
    "        new_array = values[mask]\n",
    "\n",
    "        left, right = min(new_array), max(new_array)\n",
    "\n",
    "        return Interval(left, right, True, True)\n",
    "        \n",
    "            \n",
    "    def test_membership(self, value):\n",
    "        return self.__membership_func(value)\n",
    "    \n",
    "    def isin(self, value):\n",
    "        return self.test_membership([value])[0]\n",
    "\n",
    "    def overlaps_with(self, other):\n",
    "        return self.isin(other.minval) or self.isin(other.maxval) or other.isin(self.minval) or other.isin(self.maxval)\n",
    "        \n",
    "\n",
    "    def string(self):\n",
    "        return \"{}{};{}{}\".format(self.left_bracket, self.minval, self.maxval, self.right_bracket)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Interval[{}{};{}{}]\".format(self.left_bracket, self.minval, self.maxval, self.right_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_intervalfunc(minv, maxv, left_inclusivity, right_inclusivity):\n",
    "    def inner_func(value):\n",
    "        if greaterthan(value, minv, left_inclusivity) and lesserthan(value, maxv, right_inclusivity):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return inner_func\n",
    "        \n",
    "def greaterthan(a, b, inclusivity):\n",
    "    if inclusivity:\n",
    "        if a >= b: return True\n",
    "    elif a > b: return True\n",
    "    \n",
    "    return False\n",
    "        \n",
    "def lesserthan(a, b, inclusivity):\n",
    "    if inclusivity:\n",
    "        if a <= b: return True\n",
    "    elif a < b: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "class Interval:\n",
    "\n",
    "    def __init__(self, minval, maxval, left_inclusive, right_inclusive):\n",
    "        self.minval = minval\n",
    "        self.maxval = maxval\n",
    "        self.left_inclusive = left_inclusive\n",
    "        self.right_inclusive = right_inclusive\n",
    "        \n",
    "        \n",
    "        self.left_bracket = \"<\" if left_inclusive else \"(\"\n",
    "        self.right_bracket = \">\" if right_inclusive else \")\"\n",
    "        \n",
    "        self.__membership_func = np.vectorize(\n",
    "            make_intervalfunc(self.minval, self.maxval, self.left_inclusive, self.right_inclusive)\n",
    "        )\n",
    "            \n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(repr(self))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "            \n",
    "    def refit(self, vals):\n",
    "        \"\"\"refit values to a finer grid\n",
    "        \"\"\"\n",
    "        values = np.array(vals)\n",
    "        \n",
    "        mask = self.test_membership(values)\n",
    "        new_array = values[mask]\n",
    "\n",
    "        left, right = min(new_array), max(new_array)\n",
    "\n",
    "        return Interval(left, right, True, True)\n",
    "        \n",
    "            \n",
    "    def test_membership(self, value):\n",
    "        return self.__membership_func(value)\n",
    "    \n",
    "    def isin(self, value):\n",
    "        return self.test_membership([value])[0]\n",
    "\n",
    "    def overlaps_with(self, other):\n",
    "        return self.isin(other.minval) or self.isin(other.maxval) or other.isin(self.minval) or other.isin(self.maxval)\n",
    "        \n",
    "\n",
    "    def string(self):\n",
    "        return \"{}{};{}{}\".format(self.left_bracket, self.minval, self.maxval, self.right_bracket)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Interval[{}{};{}{}]\".format(self.left_bracket, self.minval, self.maxval, self.right_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interval[<1.2;2.3>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class IntervalReader():\n",
    "    \n",
    "    \n",
    "    interval_regex = re.compile(\"(<|\\()(\\d+(?:\\.(?:\\d)+)?);(\\d+(?:\\.(?:\\d)+)?)(\\)|>)\")\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # opened interval brackets\n",
    "        self.__open_bracket = \"(\", \")\"\n",
    "        \n",
    "        # closed interval brackets\n",
    "        self.__closed_bracket = \"<\", \">\"\n",
    "        \n",
    "        # negative and positive infinity symbol,\n",
    "        # e.g. -inf, +inf\n",
    "        self.__infinity_symbol = \"-inf\", \"+inf\"\n",
    "        \n",
    "        # decimal separator, e.g. \".\", \",\"\n",
    "        self.__decimal_separator = \".\"\n",
    "        \n",
    "        # interval members separator\n",
    "        self.__members_separator = \";\"\n",
    "        \n",
    "        self.compile_reader()\n",
    "        \n",
    "        \n",
    "    def compile_reader(self):\n",
    "\n",
    "        left_bracket_open = re.escape(self.open_bracket[0])\n",
    "        left_bracket_closed = re.escape(self.closed_bracket[0])\n",
    "        \n",
    "        right_bracket_open = re.escape(self.open_bracket[1])\n",
    "        right_braket_closed = re.escape(self.closed_bracket[1])\n",
    "        \n",
    "        # e.g. (   <    |   \\(    ) \n",
    "        #      (   {}   |   {}    )\n",
    "        left_bracket_regex_string = \"({}|{})\".format(\n",
    "            left_bracket_open,\n",
    "            left_bracket_closed\n",
    "        )\n",
    "        \n",
    "        # e.g. (   >   |   \\)    ) \n",
    "        #      (   {}   |   {}    )\n",
    "        right_bracket_regex_string = \"({}|{})\".format(\n",
    "            right_bracket_open,\n",
    "            right_braket_closed\n",
    "        )\n",
    "        \n",
    "        # ((   \\d+  (?:  \\.   (?:\\d)+  )?   )|-inf)\n",
    "        # (   \\d+  (?:  {}   (?:\\d)+  )?   )\n",
    "        left_number_regex_string = \"(\\-?\\d+(?:{}(?:\\d)+)?|{})\".format(\n",
    "            re.escape(self.decimal_separator),\n",
    "            re.escape(self.infinity_symbol[0]),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # ((   \\d+  (?:  \\.   (?:\\d)+  )?   )|+inf)\n",
    "        # (   \\d+  (?:  {}   (?:\\d)+  )?   )\n",
    "        right_number_regex_string = \"(\\-?\\d+(?:{}(?:\\d)+)?|{})\".format(\n",
    "            re.escape(self.decimal_separator),\n",
    "            re.escape(self.infinity_symbol[1]),\n",
    "        )\n",
    "        \n",
    "        members_separator_regex = \"{}\".format(\n",
    "            re.escape(self.members_separator)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        interval_regex_string = \"{}{}{}{}{}\".format(\n",
    "            left_bracket_regex_string,\n",
    "            left_number_regex_string,\n",
    "            members_separator_regex,\n",
    "            right_number_regex_string,\n",
    "            right_bracket_regex_string\n",
    "        )\n",
    "        \n",
    "        self.__interval_regex = re.compile(interval_regex_string)\n",
    "        \n",
    "        \n",
    "    def read(self, interval_string):\n",
    "        # returns array of results, take first member\n",
    "        args = self.__interval_regex.findall(interval_string)[0]\n",
    "        \n",
    "        left_bracket, minval, maxval, right_bracket = args\n",
    "        \n",
    "        left_inclusive = True if left_bracket == self.closed_bracket[0] else False\n",
    "        right_inclusive = True if right_bracket == self.closed_bracket[1] else False\n",
    "        \n",
    "        \n",
    "        minval_final = float(minval) if minval != self.infinity_symbol[0] else np.NINF \n",
    "        maxval_final = float(maxval) if maxval != self.infinity_symbol[1] else np.PINF\n",
    "        \n",
    "        interval = Interval(\n",
    "            minval_final,\n",
    "            maxval_final,\n",
    "            left_inclusive,\n",
    "            right_inclusive\n",
    "        )\n",
    "        \n",
    "        return interval\n",
    "      \n",
    "        \n",
    "    # boilerplate getter/setter code    \n",
    "    \n",
    "    @property\n",
    "    def open_bracket(self):\n",
    "        return self.__open_bracket\n",
    "    \n",
    "    @open_bracket.setter\n",
    "    def open_bracket(self, val):\n",
    "        self.__open_bracket = val\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def closed_bracket(self):\n",
    "        return self.__closed_bracket\n",
    "    \n",
    "    @closed_bracket.setter\n",
    "    def closed_bracket(self, val):\n",
    "        self.__closed_bracket = val\n",
    "        return self\n",
    "        \n",
    "    @property\n",
    "    def infinity_symbol(self):\n",
    "        return self.__infinity_symbol\n",
    "    \n",
    "    @infinity_symbol.setter\n",
    "    def infinity_symbol(self, val):\n",
    "        self.__infinity_symbol = val\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def decimal_separator(self):\n",
    "        return self.__decimal_separator\n",
    "    \n",
    "    @decimal_separator.setter\n",
    "    def decimal_separator(self, val):\n",
    "        self.__decimal_separator = val\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def members_separator(self):\n",
    "        return self.__members_separator\n",
    "    \n",
    "    @members_separator.setter\n",
    "    def members_separator(self, val):\n",
    "        self.__members_separator = val\n",
    "        return self\n",
    "    \n",
    "    \n",
    "        \n",
    "interval_reader = IntervalReader()\n",
    "\n",
    "interval_reader.compile_reader()\n",
    "\n",
    "interval_reader.read(\"<1.2;2.3>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class QuantitativeCAR:\n",
    "    \n",
    "    interval_reader = IntervalReader()\n",
    "    \n",
    "    def __init__(self, rule):\n",
    "        self.antecedent = self.__create_intervals_from_antecedent(rule.antecedent)\n",
    "        self.consequent = copy.copy(rule.consequent)\n",
    "        \n",
    "        self.confidence = rule.confidence\n",
    "        self.support = rule.support\n",
    "        self.rulelen = rule.rulelen\n",
    "        self.rid = rule.rid\n",
    "        \n",
    "        # property which indicates wheter the rule was extended or not\n",
    "        self.was_extended = False\n",
    "        # literal which extended the rule\n",
    "        self.extension_literal = None\n",
    "\n",
    "        self.interval_reader = QuantitativeCAR.interval_reader\n",
    "        \n",
    "        \n",
    "    def __create_intervals_from_antecedent(self, antecedent):\n",
    "        interval_antecedent = []\n",
    "        \n",
    "        for literal in antecedent:\n",
    "            attribute, value = literal\n",
    "            \n",
    "            # catch error if attribute is ordinal\n",
    "            try:\n",
    "                interval = QuantitativeCAR.interval_reader.read(value)\n",
    "            \n",
    "                interval_antecedent.append((attribute, interval))\n",
    "            except:\n",
    "                interval_antecedent.append((attribute, value))\n",
    "        \n",
    "        \n",
    "        return self.__sort_antecedent(interval_antecedent)\n",
    "    \n",
    "    \n",
    "    def __sort_antecedent(self, antecedent):\n",
    "        return sorted(antecedent)\n",
    "    \n",
    "    \n",
    "    def update_properties(self, quant_dataframe):\n",
    "        \"\"\"updates rule properties using instance\n",
    "        of QuantitativeDataFrame\n",
    "        \n",
    "        properties:\n",
    "            support, confidence, rulelen\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if quant_dataframe.__class__.__name__ != \"QuantitativeDataFrame\":\n",
    "            raise Exception(\n",
    "                \"type of quant_dataframe must be QuantitativeDataFrame\"\n",
    "            )\n",
    "            \n",
    "        \n",
    "        support, confidence = quant_dataframe.calculate_rule_statistics(self)\n",
    "        \n",
    "        self.support = support\n",
    "        self.confidence = confidence\n",
    "        # length of antecedent + length of consequent\n",
    "        self.rulelen = len(self.antecedent) + 1\n",
    "        \n",
    "    \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "\n",
    "        copied = copy.copy(self)\n",
    "        copied.antecedent = copy.deepcopy(self.antecedent)\n",
    "        copied.consequent = copy.deepcopy(self.consequent)\n",
    "\n",
    "        return copied\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        ant = self.antecedent\n",
    "\n",
    "        ant_string_arr = []\n",
    "        for key, val in ant:\n",
    "            if type(val) == str:\n",
    "                ant_string_arr.append(\"{}={}\".format(key, val))\n",
    "            else:\n",
    "                ant_string_arr.append(\"{}={}\".format(key, val.string()))\n",
    "\n",
    "        ant_string = \"{\" + \",\".join(ant_string_arr) + \"}\"\n",
    "        \n",
    "        args = [\n",
    "            ant_string,\n",
    "            \"{\" + self.consequent.string() + \"}\",\n",
    "            self.support,\n",
    "            self.confidence,\n",
    "            self.rulelen,\n",
    "            self.rid\n",
    "        ]\n",
    "        \n",
    "        text = \"CAR {} => {} sup: {:.2f} conf: {:.2f} len: {}, id: {}\".format(*args)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        \"\"\"\n",
    "        precedence operator. Determines if this rule\n",
    "        has higher precedence. Rules are sorted according\n",
    "        to their confidence, support, length and id.\n",
    "        \"\"\"\n",
    "        if (self.confidence > other.confidence):\n",
    "            return True\n",
    "        elif (self.confidence == other.confidence and\n",
    "              self.support > other.support):\n",
    "            return True\n",
    "        elif (self.confidence == other.confidence and\n",
    "              self.support == other.support and\n",
    "              self.rulelen < other.rulelen):\n",
    "            return True\n",
    "        elif(self.confidence == other.confidence and\n",
    "              self.support == other.support and\n",
    "              self.rulelen == other.rulelen and\n",
    "              self.rid < other.rid):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"\n",
    "        rule precedence operator\n",
    "        \"\"\"\n",
    "        return not self > other\n",
    "    \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.rid == other.rid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "class LiteralCache:\n",
    "    \"\"\"class which stores literals\n",
    "    and corresponding truth values\n",
    "    e.g. [\n",
    "        \"food=banana\": [True, True, False, False, True],\n",
    "        \"food=apple\" : [True, True, True, True, False]\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__cache = {}\n",
    "\n",
    "    def insert(self, literal, truth_values):\n",
    "        self.__cache[literal] = truth_values\n",
    "        \n",
    "    def get(self, literal):\n",
    "        return self.__cache[literal]\n",
    "        \n",
    "    def __contains__(self, literal):\n",
    "        \"\"\"function for using in\n",
    "        on LiteralCache object\n",
    "        \"\"\"\n",
    "        \n",
    "        return literal in self.__cache.keys()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class QuantitativeDataFrame:\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        if type(dataframe) != pandas.DataFrame:\n",
    "            raise Exception(\"type of dataframe must be pandas.dataframe\")\n",
    "        \n",
    "        \n",
    "        self.__dataframe = dataframe\n",
    "        self.__dataframe.iloc[:,-1] = self.__dataframe.iloc[:,-1].astype(str)\n",
    "        \n",
    "        # sorted and unique columns of the dataframe\n",
    "        # saved as a numpy array\n",
    "        self.__preprocessed_columns = self.__preprocess_columns(dataframe)\n",
    "        \n",
    "        \n",
    "        # literal cache for computing rule statistics\n",
    "        # - support and confidence\n",
    "        self.__literal_cache = LiteralCache()\n",
    "\n",
    "        # so that it doesn't have to be computed over and over\n",
    "        self.size = dataframe.index.size\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        return self.__dataframe\n",
    "    \n",
    "    \n",
    "    def column(self, colname):\n",
    "        return self.__preprocessed_columns[colname]\n",
    "    \n",
    "    \n",
    "    def mask(self, vals):\n",
    "        return self.__dataframe[vals]\n",
    "    \n",
    "    \n",
    "    def find_covered_by_antecedent_mask(self, antecedent):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            mask - an array of boolean values indicating which instances\n",
    "            are covered by antecedent\n",
    "        \"\"\"\n",
    "        \n",
    "        # todo: compute only once to make function faster\n",
    "        dataset_size = self.__dataframe.index.size\n",
    "\n",
    "        cummulated_mask = np.ones(dataset_size).astype(bool)\n",
    "        \n",
    "        for literal in antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "\n",
    "        return cummulated_mask\n",
    "    \n",
    "    \n",
    "    def find_covered_by_literal_mask(self, literal):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            mask - an array of boolean values indicating which instances\n",
    "            are covered by literal\n",
    "        \"\"\"\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "    \n",
    "    \n",
    "    def find_covered_by_rule_mask(self, rule):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            covered_by_antecedent_mask:\n",
    "                - array of boolean values indicating which\n",
    "                dataset rows satisfy antecedent\n",
    "                \n",
    "            covered_by_consequent_mask:\n",
    "                - array of boolean values indicating which\n",
    "                dataset rows satisfy conseqeunt\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset_size = self.__dataframe.index.size\n",
    "        \n",
    "        # initialize a mask filled with True values\n",
    "        # it will get modified as futher literals get\n",
    "        # tested\n",
    "        \n",
    "        # for optimization - create cummulated mask once\n",
    "        # in constructor\n",
    "        cummulated_mask = np.array([True] * dataset_size)\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "            \n",
    "            \n",
    "        \n",
    "        instances_satisfying_antecedent_mask = cummulated_mask\n",
    "        instances_satisfying_consequent_mask = self.__get_consequent_coverage_mask(rule)\n",
    "        instances_satisfying_consequent_mask = instances_satisfying_consequent_mask.reshape(dataset_size)\n",
    "        \n",
    "        return instances_satisfying_antecedent_mask, instances_satisfying_consequent_mask\n",
    "        \n",
    "        \n",
    "    \n",
    "    def calculate_rule_statistics(self, rule):\n",
    "        \"\"\"calculates rule's confidence and\n",
    "        support using efficient numpy functions\n",
    "        \n",
    "        \n",
    "        returns:\n",
    "        --------\n",
    "        \n",
    "            support:\n",
    "                float\n",
    "            \n",
    "            confidence:\n",
    "                float\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset_size = self.__dataframe.index.size\n",
    "        \n",
    "        # initialize a mask filled with True values\n",
    "        # it will get modified as futher literals get\n",
    "        # tested\n",
    "        \n",
    "        # for optimization - create cummulated mask once\n",
    "        # in constructor\n",
    "        cummulated_mask = np.array([True] * dataset_size)\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            # the column that concerns the\n",
    "            # iterated attribute\n",
    "            # instead of pandas.Series, grab the ndarray\n",
    "            # using values attribute\n",
    "            relevant_column = self.__dataframe[[attribute]].values.reshape(dataset_size)\n",
    "            \n",
    "            # this tells us which instances satisfy the literal\n",
    "            current_mask = self.get_literal_coverage(literal, relevant_column)\n",
    "            \n",
    "            # add cummulated and current mask using logical AND\n",
    "            cummulated_mask &= current_mask\n",
    "            \n",
    "        \n",
    "        instances_satisfying_antecedent = self.__dataframe[cummulated_mask].index\n",
    "        instances_satisfying_antecedent_count = instances_satisfying_antecedent.size\n",
    "        \n",
    "        # using cummulated mask to filter out instances that satisfy consequent\n",
    "        # but do not satisfy antecedent\n",
    "        instances_satisfying_consequent_mask = self.__get_consequent_coverage_mask(rule)\n",
    "        instances_satisfying_consequent_mask = instances_satisfying_consequent_mask.reshape(dataset_size)\n",
    "        \n",
    "        instances_satisfying_consequent_and_antecedent = self.__dataframe[\n",
    "            instances_satisfying_consequent_mask & cummulated_mask\n",
    "        ].index\n",
    "        \n",
    "        instances_satisfying_consequent_and_antecedent_count = instances_satisfying_consequent_and_antecedent.size\n",
    "        instances_satisfying_consequent_count = self.__dataframe[instances_satisfying_consequent_mask].index.size\n",
    "        \n",
    "        # instances satisfying consequent both antecedent and consequent \n",
    "\n",
    "        support = instances_satisfying_antecedent_count / dataset_size\n",
    "        \n",
    "        confidence = 0\n",
    "        if instances_satisfying_antecedent_count != 0:\n",
    "            confidence = instances_satisfying_consequent_and_antecedent_count / instances_satisfying_antecedent_count\n",
    "        \n",
    "        return support, confidence\n",
    "    \n",
    "    \n",
    "    def __get_consequent_coverage_mask(self, rule):\n",
    "        consequent = rule.consequent\n",
    "        attribute, value = consequent\n",
    "\n",
    "        class_column = self.__dataframe[[attribute]].values\n",
    "        class_column = class_column.astype(str)\n",
    "\n",
    "        literal_key = \"{}={}\".format(attribute, value)\n",
    "\n",
    "        mask = []\n",
    "        \n",
    "        if literal_key in self.__literal_cache:\n",
    "            mask = self.__literal_cache.get(literal_key)\n",
    "        else:\n",
    "            mask = class_column == value\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def get_literal_coverage(self, literal, values):\n",
    "        \"\"\"returns mask which describes the instances that\n",
    "        satisfy the interval\n",
    "        \n",
    "        function uses cached results for efficiency\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(values) != np.ndarray:\n",
    "            raise Exception(\"Type of values must be numpy.ndarray\")\n",
    "            \n",
    "        mask = []\n",
    "        \n",
    "        attribute, interval = literal\n",
    "        \n",
    "        literal_key = \"{}={}\".format(attribute, interval)\n",
    "        \n",
    "        # check if the result is already cached, otherwise\n",
    "        # calculate and save the result\n",
    "        if literal_key in self.__literal_cache:\n",
    "            mask = self.__literal_cache.get(literal_key)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "            if type(interval) == str:\n",
    "                mask = np.array([ val == interval for val in values ])\n",
    "            else:\n",
    "                mask = interval.test_membership(values)\n",
    "            \n",
    "            self.__literal_cache.insert(literal_key, mask)\n",
    "            \n",
    "        # reshape mask into single dimension\n",
    "        mask = mask.reshape(values.size)\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def __preprocess_columns(self, dataframe):\n",
    "        \n",
    "        # covert to dict\n",
    "        # column -> list\n",
    "        # need to convert it to numpy array\n",
    "        dataframe_dict = dataframe.to_dict(orient=\"list\")\n",
    "        \n",
    "        dataframe_ndarray = {}\n",
    "        \n",
    "        \n",
    "        for column, value_list in dataframe_dict.items():\n",
    "            transformed_list = np.sort(np.unique(value_list))\n",
    "            dataframe_ndarray[column] = transformed_list\n",
    "            \n",
    "        return dataframe_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantitativeClassifier:\n",
    "    import math\n",
    "    def __init__(self, rules, default_class):\n",
    "        self.rules = rules\n",
    "        self.default_class = default_class\n",
    "            \n",
    "\n",
    "    def rule_model_accuracy(self, quantitative_dataframe, ground_truth, df, QCBA):\n",
    "        predicted = self.predict(quantitative_dataframe)\n",
    "        if len(df[0][0][0]) == 15 or len(df) == 1007:\n",
    "            return prediction_class_model(df, QCBA)\n",
    "        return accuracy_score(predicted, ground_truth) \n",
    "\n",
    "    def predict(self, quantitative_dataframe):\n",
    "        predicted_classes = []\n",
    "    \n",
    "        for _, row in quantitative_dataframe.dataframe.iterrows():\n",
    "            appended = False\n",
    "            for rule in self.rules:\n",
    "                antecedent_dict = dict(rule.antecedent)  \n",
    "                counter = True\n",
    "\n",
    "                for name, value in row.iteritems():\n",
    "                    if name in antecedent_dict:\n",
    "                        interval = antecedent_dict[name]\n",
    "\n",
    "                        if type(interval) == str:\n",
    "                            counter &= interval == value\n",
    "                        else:\n",
    "                            result = interval.isin(value)\n",
    "                            counter &= result\n",
    "\n",
    "                if counter:\n",
    "                    _, predicted_class = rule.consequent\n",
    "                    predicted_classes.append(predicted_class)\n",
    "                    appended = True\n",
    "                    break\n",
    "                    \n",
    "            if not appended:\n",
    "                predicted_classes.append(self.default_class)\n",
    "\n",
    "                    \n",
    "        return predicted_classes            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCBATransformation:\n",
    "\n",
    "\n",
    "    def __init__(self, quantitative_dataset, transaction_based_drop=True):\n",
    "        self.transaction_based_drop = transaction_based_drop\n",
    "\n",
    "        self.dataset = quantitative_dataset\n",
    "\n",
    "        self.refitter = RuleRefitter(self.dataset)\n",
    "        self.literal_pruner = RuleLiteralPruner(self.dataset)\n",
    "        self.trimmer = RuleTrimmer(self.dataset)\n",
    "        self.extender = RuleExtender(self.dataset)\n",
    "        self.post_pruner = RulePostPruner(self.dataset)\n",
    "        self.overlap_pruner = RuleOverlapPruner(self.dataset)\n",
    "\n",
    "\n",
    "    def transform(self, rules, transformation_dict={}):\n",
    "\n",
    "        if not transformation_dict:\n",
    "            print(\"applying all transformations\")\n",
    "            refitted = self.refitter.transform(rules)\n",
    "            literal_pruned = self.literal_pruner.transform(refitted)\n",
    "            trimmed = self.trimmer.transform(literal_pruned)\n",
    "            extended = self.extender.transform(trimmed)\n",
    "            post_pruned, default_class = self.post_pruner.transform(extended)\n",
    "            overlap_pruned = self.overlap_pruner.transform(post_pruned, default_class, transaction_based=self.transaction_based_drop)\n",
    "        \n",
    "        else:\n",
    "            print(\"applying selected transformations\")\n",
    "            transformed_rules = rules\n",
    " \n",
    "            if transformation_dict.get(\"refitting\", False):\n",
    "                print(\"refitting\")\n",
    "                transformed_rules = self.refitter.transform(transformed_rules)\n",
    "            if transformation_dict.get(\"literal_pruning\", False):\n",
    "                print(\"literal pruning\")\n",
    "                transformed_rules = self.literal_pruner.transform(transformed_rules)\n",
    "            if transformation_dict.get(\"trimming\", False):\n",
    "                print(\"trimming\")\n",
    "                transformed_rules = self.trimmer.transform(transformed_rules)\n",
    "            if transformation_dict.get(\"extension\", False):\n",
    "                print(\"extending\")\n",
    "                transformed_rules = self.extender.transform(transformed_rules)\n",
    "\n",
    "            print(\"post pruning\")\n",
    "            try:\n",
    "                transformed_rules, default_class = self.post_pruner.transform(transformed_rules)\n",
    "            except:\n",
    "                print(\"overlap pruning\")\n",
    "                return None\n",
    "            if transformation_dict.get(\"overlap_pruning\", False):\n",
    "                print(\"overlap pruning\")\n",
    "                transaction_based = transformation_dict[\"transaction_based_drop\"]\n",
    "                transformed_rules = self.overlap_pruner.transform(transformed_rules, default_class, transaction_based=transaction_based)\n",
    "            return transformed_rules, default_class\n",
    "\n",
    "        return overlap_pruned, default_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCBA:\n",
    "    import math\n",
    "    def __init__(self, quantitative_dataset, cba_rule_model=None, rules=None):\n",
    "        if rules and cba_rule_model:\n",
    "            raise Exception(\"rules and cba_rule_model cannot be specified together\")\n",
    "\n",
    "        if not rules and not cba_rule_model:\n",
    "            raise Exception(\"either rules and cba_rule_model need to be specified\")\n",
    "\n",
    "        self.quantitative_dataset = quantitative_dataset\n",
    "        self.__quant_rules = None\n",
    "\n",
    "        if cba_rule_model:\n",
    "            self.__quant_rules = [ QuantitativeCAR(r) for r in cba_rule_model.clf.rules ]\n",
    "        if rules:\n",
    "            self.__quant_rules = [ QuantitativeCAR(r) for r in rules ]\n",
    "         \n",
    "\n",
    "        self.qcba_transformation = QCBATransformation(quantitative_dataset)\n",
    "\n",
    "        self.clf = None\n",
    "\n",
    "    def fit(\n",
    "            self, \n",
    "            refitting=True,\n",
    "            literal_pruning=True,\n",
    "            trimming=True,\n",
    "            extension=True,\n",
    "            overlap_pruning=True,\n",
    "            transaction_based_drop=True\n",
    "        ):\n",
    "\n",
    "        transformation_dict = {\n",
    "            \"refitting\": refitting,\n",
    "            \"literal_pruning\": literal_pruning,\n",
    "            \"trimming\": trimming,\n",
    "            \"extension\": extension,\n",
    "            \"overlap_pruning\": overlap_pruning,\n",
    "            \"transaction_based_drop\": transaction_based_drop\n",
    "        }\n",
    "\n",
    "        try:  \n",
    "            transformed_rules, default_class = self.qcba_transformation.transform(self.__quant_rules, transformation_dict)\n",
    "            self.clf = QuantitativeClassifier(transformed_rules, default_class)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        return self.clf\n",
    "    \n",
    "    def param(refitting,literal_pruning,trimming,extension,overlap_pruning,transaction_based_drop, accuracy):\n",
    "        return accuracy\n",
    "        \n",
    "    def score(self, QCBA, quantitative_dataset, df):\n",
    "        actual = quantitative_dataset.dataframe.iloc[:, -1]\n",
    "        if len(df) == 103 or len(df) == 120 or len(df) == 280  or len(df) == 6499 or len(df) == 67:\n",
    "            return prediction_class_model(df, QCBA)\n",
    "        return self.clf.rule_model_accuracy(quantitative_dataset, actual, df, QCBA)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "425409b728387ae2650caeeea5cef9cde765651dd86e550a51171ebb4a08778f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
